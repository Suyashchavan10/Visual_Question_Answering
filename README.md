# Visual_Question_Answering
Developed a VQA model using BERT for textual embeddings and Vision Transformer (ViT) for visual embeddings, without multimodal models like CLIP or BLIP. Compared traditional fine-tuning with Low-Rank Adaptation (LoRA), finding LoRA enhances performance and reduces training time.
