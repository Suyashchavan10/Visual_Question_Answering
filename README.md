Visual Question Answering (VQA) combines computer vision and
 natural language processing to answer questions about images.
 In this study, we develop a VQA model using BERT for textual
 embeddings and Vision Transformer (ViT) for visual embeddings,
 without relying on integrated multimodal models like CLIP or BLIP.
Our primary focus is to compare the traditional fine-tuning
 approach with Low-Rank Adaptation (LoRA) in terms of accuracy,
 F1 score, precision, recall, and training time. The findings suggest
 that LoRA not only enhances the performance of the VQA model
 but also significantly reduces the training duration, showcasing its
 potential for more efficient model optimization.
