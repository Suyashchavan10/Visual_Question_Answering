{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8442807,"sourceType":"datasetVersion","datasetId":5030156},{"sourceId":8430164,"sourceType":"datasetVersion","datasetId":5020279},{"sourceId":8444421,"sourceType":"datasetVersion","datasetId":5031597},{"sourceId":8444530,"sourceType":"datasetVersion","datasetId":5031675},{"sourceId":8444698,"sourceType":"datasetVersion","datasetId":5031796},{"sourceId":8444795,"sourceType":"datasetVersion","datasetId":5031872},{"sourceId":8445001,"sourceType":"datasetVersion","datasetId":5032005}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import BertModel, BertTokenizer, ViTImageProcessor, ViTModel\nimport torch\nfrom torchinfo import summary\nfrom torch import nn\nfrom torch.nn import Transformer, TransformerDecoder, TransformerDecoderLayer, TransformerEncoder, TransformerEncoderLayer","metadata":{"execution":{"iopub.status.busy":"2024-05-18T01:55:15.156883Z","iopub.execute_input":"2024-05-18T01:55:15.157511Z","iopub.status.idle":"2024-05-18T01:55:15.164635Z","shell.execute_reply.started":"2024-05-18T01:55:15.157479Z","shell.execute_reply":"2024-05-18T01:55:15.163601Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T01:55:17.535936Z","iopub.execute_input":"2024-05-18T01:55:17.536303Z","iopub.status.idle":"2024-05-18T01:55:17.572760Z","shell.execute_reply.started":"2024-05-18T01:55:17.536270Z","shell.execute_reply":"2024-05-18T01:55:17.571544Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"class TextTokenizer(torch.nn.Module):\n    def __init__(\n        self,\n        text_tokenizer=BertTokenizer,\n        max_length=25  # Add a max_length parameter\n    ):\n        super().__init__()\n        self.text_tokenizer = text_tokenizer.from_pretrained('bert-base-uncased')\n        self.max_length = max_length  # Store the max_length\n\n    def forward(self, input_question, padding='max_length', truncation=True):\n        tokens = self.text_tokenizer(input_question, return_tensors='pt', \n                                     padding=padding, truncation=truncation, \n                                     max_length=self.max_length).to(device)  # Use max_length\n\n        return tokens\n\nclass ImageProcessor(torch.nn.Module):\n    def __init__(\n        self,\n        image_model_processor=ViTImageProcessor\n    ):\n\n        super().__init__()\n        self.image_model_processor = image_model_processor.from_pretrained('google/vit-base-patch16-224-in21k')\n\n    def forward(self, image):\n        image = self.image_model_processor(image, return_tensors='pt').to(device)\n\n        return image\n\nclass TextEmbedding(torch.nn.Module):\n    def __init__(\n        self,\n        text_model=BertModel,\n    ):\n        super().__init__()\n        self.text_model = text_model.from_pretrained('bert-base-uncased').to(device)\n\n\n    def forward(self, tokens):\n        text_output = self.text_model(input_ids=tokens.input_ids, attention_mask=tokens.attention_mask)\n        text_output = text_output.last_hidden_state     # CLS token from the last layer\n\n        return text_output\n\n\nclass ImageEmbedding(torch.nn.Module):\n    def __init__(\n            self, \n            image_model=ViTModel\n        ):\n        \n        super().__init__()\n        self.image_model = image_model.from_pretrained('google/vit-base-patch16-224-in21k').to(device)\n\n\n    def forward(self, image):\n        image_output = self.image_model(pixel_values=image.pixel_values).last_hidden_state\n\n        return image_output","metadata":{"execution":{"iopub.status.busy":"2024-05-18T01:55:24.716109Z","iopub.execute_input":"2024-05-18T01:55:24.716480Z","iopub.status.idle":"2024-05-18T01:55:24.728542Z","shell.execute_reply.started":"2024-05-18T01:55:24.716450Z","shell.execute_reply":"2024-05-18T01:55:24.727520Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class VQAModel(nn.Module):\n    def __init__(\n        self,\n        dim_model = 768,      # image and text embeddings concatenated\n        nhead = 12,                    # No. of Attention heads\n        num_layers = 1,               # No. of encoder layers\n        num_classes = 8000\n    ):\n        super().__init__()\n        self.text_embedder = TextEmbedding()\n        self.image_embedder = ImageEmbedding()\n\n        encoder_layers = TransformerEncoderLayer(d_model=dim_model, nhead=nhead)\n        self.transformerEncoder = TransformerEncoder(encoder_layer=encoder_layers, num_layers=num_layers).to(device)\n\n        self.classifier = nn.Linear(dim_model, num_classes).to(device)\n        self.softmax = nn.Softmax(dim=1)\n\n        # self.target_transform = nn.Linear(768, dim_model).to(device)\n        # decoder_layers = TransformerDecoderLayer(dim_model, nhead)\n        # self.transformerDecoder = TransformerDecoder(decoder_layers, num_layers).to(device)\n\n\n    def forward(self, questions, images):\n        question_embedding = self.text_embedder(questions)\n        image_embedding = self.image_embedder(images)\n\n        embeddings = torch.cat((question_embedding, image_embedding), dim=1)\n        embeddings = embeddings.permute(1, 0, 2)  # (seq, batch, feature)\n        output = self.transformerEncoder(embeddings)\n\n        cls_output = output[0, :, :]\n\n        logits = self.classifier(cls_output)\n        output = self.softmax(logits)\n\n        return output\n","metadata":{"execution":{"iopub.status.busy":"2024-05-18T01:55:28.921040Z","iopub.execute_input":"2024-05-18T01:55:28.921403Z","iopub.status.idle":"2024-05-18T01:55:28.931669Z","shell.execute_reply.started":"2024-05-18T01:55:28.921372Z","shell.execute_reply":"2024-05-18T01:55:28.930753Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"HuggingFaceM4/VQAv2\", split=\"train\")","metadata":{"execution":{"iopub.status.busy":"2024-05-18T01:55:37.556317Z","iopub.execute_input":"2024-05-18T01:55:37.557257Z","iopub.status.idle":"2024-05-18T02:50:02.209533Z","shell.execute_reply.started":"2024-05-18T01:55:37.557208Z","shell.execute_reply":"2024-05-18T02:50:02.208742Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for HuggingFaceM4/VQAv2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/HuggingFaceM4/VQAv2\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.36k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c278f911a65649288e08585104469bba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/352 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a50738149d424b27aa553e018963b016"}},"metadata":{}},{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/7.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb1e3cf3ee2e4fc49fd7d0ceb6d31f69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/3.49M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54db5ed591e945f59232dce67022c0c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/8.97M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"719c72d3b0df45c8bb9b8e45bdace2d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/21.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c436838375e491c85669183611bf512"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/10.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7ce9290457f423ba3ab6679a25b75d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/13.5G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80df7342bf944483b09bb60caff5314b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/6.65G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b92c71b6627c428a96a97886f8032a1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/13.3G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff2ca40673fc4e22aee42befad3bcf35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8652681c1314117ac3f5080dfe329ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91efca26213b4bbda4587c1c4088686e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating testdev split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6a70b2a33b448858af1b4f0fc99576b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed27ec43f27945918203d061402d8ecc"}},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n\ntrain_dataset = dataset['train']\n\ntrain_df = pd.read_csv('/kaggle/input/vqdata/vqa_train_dataset.csv')\nval_df = pd.read_csv('/kaggle/input/vqdata/vqa_val_dataset.csv')\n\ntrain_df = train_df[~train_df['answers'].isna()]\nval_df = val_df[~val_df['answers'].isna()]","metadata":{"execution":{"iopub.status.busy":"2024-05-18T03:07:44.295382Z","iopub.execute_input":"2024-05-18T03:07:44.296103Z","iopub.status.idle":"2024-05-18T03:07:44.606265Z","shell.execute_reply.started":"2024-05-18T03:07:44.296069Z","shell.execute_reply":"2024-05-18T03:07:44.605369Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom torchvision.transforms import Compose, Resize, CenterCrop, ToTensor\n\nclass VQADataset(Dataset):\n    def __init__(self, dataframe, image_dataset):\n        self.dataframe = dataframe\n        self.image_dataset = image_dataset\n        self.text_tokenizer = TextTokenizer()\n        self.image_processor = ImageProcessor()\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        \n        ind = int(row['index'])\n        image = self.image_dataset[ind]['image']\n        question = row['question']\n        answer = row['answers']\n        \n        # sanity check        \n        assert self.image_dataset[ind]['question'] == question, \"Mismatching training and Image data\"\n\n        \n        image = image.convert('RGB')\n        \n#         Preprocessing done in ImageEmbedder\n#         preprocess = Compose([\n#             Resize((224, 224)),\n#             CenterCrop(224),\n#             ToTensor(),\n#         ])\n#         image = preprocess(image)\n\n        # Tokenize question\n        tokens = self.text_tokenizer(question, padding='max_length', truncation=True)\n        tokens.input_ids = tokens.input_ids.squeeze()\n        tokens.attention_mask = tokens.attention_mask.squeeze()\n        image = self.image_processor(image)\n        return {\n            'image': image,\n            'questions': tokens,\n            'answer': answer\n        }\n\nbatch_size = 64\n    \n# Assuming you have separate dataframes for training and validation\nval_data = VQADataset(val_df, train_dataset)\n\n# DataLoader for training and validation\nval_dataloader = DataLoader(val_data, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T03:07:45.493684Z","iopub.execute_input":"2024-05-18T03:07:45.494524Z","iopub.status.idle":"2024-05-18T03:07:46.006772Z","shell.execute_reply.started":"2024-05-18T03:07:45.494488Z","shell.execute_reply":"2024-05-18T03:07:46.005767Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nwith open('/kaggle/input/vqdata/answers_dictionaries.pkl', 'rb') as f:\n    data = pickle.load(f)\n    id_to_answer = data['id_to_answer']\n    answer_to_id = data['answer_to_id']\n\nassert len(answer_to_id) == len(id_to_answer)\nval_model = VQAModel(num_classes=len(answer_to_id))","metadata":{"execution":{"iopub.status.busy":"2024-05-18T03:07:49.758821Z","iopub.execute_input":"2024-05-18T03:07:49.759453Z","iopub.status.idle":"2024-05-18T03:07:51.627664Z","shell.execute_reply.started":"2024-05-18T03:07:49.759422Z","shell.execute_reply":"2024-05-18T03:07:51.626614Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"# from torch.utils.tensorboard import SummaryWriter\n\nCHECKPOINT_FILE = '/kaggle/input/lora32/lora_32_final.pth'\n\nval_model = VQAModel(num_classes=len(answer_to_id))\nstate_dict = torch.load(CHECKPOINT_FILE)\n\n# val_writer = SummaryWriter('/runs/baseline_val')\n\nval_model.load_state_dict(state_dict)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T03:07:54.070976Z","iopub.execute_input":"2024-05-18T03:07:54.071335Z","iopub.status.idle":"2024-05-18T03:07:55.996681Z","shell.execute_reply.started":"2024-05-18T03:07:54.071306Z","shell.execute_reply":"2024-05-18T03:07:55.995757Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"from torchmetrics.classification import Precision, Recall, Accuracy, F1Score, AUROC\nprecision_metric = Precision(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\nrecall_metric = Recall(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\naccuracy_metric = Accuracy(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\nf1_metric = F1Score(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n# auroc_metric = AUROC(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n\ndef evaluate(preds, true):\n    p = precision_metric(preds, true)\n    r = recall_metric(preds, true)\n    a = accuracy_metric(preds, true)\n    f = f1_metric(preds, true)\n#     am = auroc_metric(preds, true)\n    \n    return {\n        \"precision\": p,\n        \"recall\": r,\n        \"accuracy\": a,\n        \"f1\": f,\n#         \"auroc\": auroc_metric\n    }\n","metadata":{"execution":{"iopub.status.busy":"2024-05-18T03:08:20.894967Z","iopub.execute_input":"2024-05-18T03:08:20.895341Z","iopub.status.idle":"2024-05-18T03:08:20.908101Z","shell.execute_reply.started":"2024-05-18T03:08:20.895308Z","shell.execute_reply":"2024-05-18T03:08:20.907212Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\nimport time\n\nbatch_no = 0\niter_val = 0\ntotal_loss = 0\nval_model.eval()\nanswer_list = torch.empty(0).to(device)\npreds_list = torch.empty(0).to(device)\n\nwith torch.no_grad():\n    start_time = time.time()\n    for batch in val_dataloader:\n        images = batch['image']\n        questions = batch['questions']\n        answers = batch['answer']\n\n        questions.input_ids = questions.input_ids.squeeze()\n        questions.attention_mask = questions.attention_mask.squeeze()\n        images.pixel_values = images.pixel_values.squeeze()\n        \n        # Forward pass\n        outputs = val_model(questions, images)\n\n        \n        answers_ids = [min(len(answer_to_id) - 1, answer_to_id[key]) for key in answers]\n        answers_ids_tensor = torch.tensor(answers_ids)\n        answers = F.one_hot(answers_ids_tensor, num_classes=len(answer_to_id)).to(outputs.dtype)\n        answers = answers.to(device)\n        \n#         answers_ids = [answer_to_id[key] for key in answers]\n#         answers = torch.zeros(batch_size, len(answer_to_id))\n#         for i in range(batch_size):\n#             answers[i, answers_ids[i]] = 1\n        \n        # Sanity check\n        assert answers.shape == outputs.shape, \"Target and Predicted shapes don't match\"\n    \n        # Compute the loss\n#         loss = criterion(outputs, answers)\n\n        \n        _, preds = torch.max(outputs, 1)\n        _, answer_inds = torch.max(answers, 1)\n        \n        assert preds.shape == answer_inds.shape, f\"Preds_shape: {preds.shape}, Answers_shape: {answer_inds.shape}\"\n        eval_met = evaluate(preds, answer_inds)\n        \n        iter_val = batch_no\n        \n        answer_list = torch.cat((answer_list, answer_inds), dim=0)\n        preds_list = torch.cat((preds_list, preds), dim=0)\n        \n        \n#         val_writer.add_scalar('Validation Loss', loss.item(), iter_val)\n#         val_writer.add_pr_curve('PR Curve', answers, outputs, iter_val)\n#         val_writer.add_scalar('Accuracy', eval_met['accuracy'], iter_val)\n#         val_writer.add_scalar('Precision', eval_met['precision'], iter_val)\n#         val_writer.add_scalar('Recall', eval_met['recall'], iter_val)\n#         val_writer.add_scalar('F1', eval_met['f1'], iter_val)\n        \n            \n        batch_no += 1\n        print(f\"Batch -> {batch_no} done -> Accu: {eval_met['accuracy']:.2f} -> Prec: {eval_met['precision']:.2f}\\r\", end=\"\")\n#         avg_accuracy += eval_met['accuracy']\n#         total_loss += loss.item()\n    end_time = time.time()\n    print(f'Time taken for prediction: {end_time - start_time} seconds')\n# # Compute the average loss and the accuracy\n# avg_loss = total_loss / len(val_dataloader)\n# accuracy = avg_accuracy / len(val_dataloader)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-18T03:08:22.257125Z","iopub.execute_input":"2024-05-18T03:08:22.257501Z","iopub.status.idle":"2024-05-18T03:19:36.375629Z","shell.execute_reply.started":"2024-05-18T03:08:22.257469Z","shell.execute_reply":"2024-05-18T03:19:36.374690Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Time taken for prediction: 674.1015012264252 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"answer_list.shape, preds_list.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-18T03:24:51.081567Z","iopub.execute_input":"2024-05-18T03:24:51.082483Z","iopub.status.idle":"2024-05-18T03:24:51.087891Z","shell.execute_reply.started":"2024-05-18T03:24:51.082448Z","shell.execute_reply":"2024-05-18T03:24:51.087044Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"(torch.Size([29994]), torch.Size([29994]))"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score","metadata":{"execution":{"iopub.status.busy":"2024-05-18T03:24:52.673183Z","iopub.execute_input":"2024-05-18T03:24:52.673544Z","iopub.status.idle":"2024-05-18T03:24:52.678066Z","shell.execute_reply.started":"2024-05-18T03:24:52.673514Z","shell.execute_reply":"2024-05-18T03:24:52.677083Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"ans_np = answer_list.cpu().numpy()\npreds_np = preds_list.cpu().numpy()\n\nf1 = f1_score(ans_np, preds_np, average='macro')\nprecision = precision_score(ans_np, preds_np, average='macro')\nrecall = recall_score(ans_np, preds_np, average='macro')\naccuracy = accuracy_score(ans_np, preds_np)\n\nprint(f'F1 Score for {CHECKPOINT_FILE}: {f1}')\nprint(f'Precision for {CHECKPOINT_FILE}: {precision}')\nprint(f'Recall for {CHECKPOINT_FILE}: {recall}')\nprint(f'Accuracy for {CHECKPOINT_FILE}: {accuracy}')","metadata":{"execution":{"iopub.status.busy":"2024-05-18T03:24:53.712877Z","iopub.execute_input":"2024-05-18T03:24:53.713647Z","iopub.status.idle":"2024-05-18T03:24:53.790207Z","shell.execute_reply.started":"2024-05-18T03:24:53.713599Z","shell.execute_reply":"2024-05-18T03:24:53.789245Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"F1 Score for /kaggle/input/lora32/lora_32_final.pth: 9.908665713403246e-05\nPrecision for /kaggle/input/lora32/lora_32_final.pth: 5.927558474881457e-05\nRecall for /kaggle/input/lora32/lora_32_final.pth: 0.00030175015087507544\nAccuracy for /kaggle/input/lora32/lora_32_final.pth: 0.1964392878575715\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]}]}