{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-17T07:29:59.406602Z","iopub.status.busy":"2024-05-17T07:29:59.406217Z","iopub.status.idle":"2024-05-17T07:30:13.214350Z","shell.execute_reply":"2024-05-17T07:30:13.213381Z","shell.execute_reply.started":"2024-05-17T07:29:59.406572Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-17 07:30:05.369536: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-17 07:30:05.369633: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-17 07:30:05.504692: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["from transformers import BertModel, BertTokenizer, ViTImageProcessor, ViTModel\n","import torch\n","from torchinfo import summary\n","from torch import nn\n","from torch.nn import Transformer, TransformerDecoder, TransformerDecoderLayer, TransformerEncoder, TransformerEncoderLayer"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T07:30:15.400021Z","iopub.status.busy":"2024-05-17T07:30:15.398940Z","iopub.status.idle":"2024-05-17T07:30:15.425802Z","shell.execute_reply":"2024-05-17T07:30:15.424706Z","shell.execute_reply.started":"2024-05-17T07:30:15.399985Z"},"trusted":true},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# device = torch.device('cpu')"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T07:30:17.367209Z","iopub.status.busy":"2024-05-17T07:30:17.366561Z","iopub.status.idle":"2024-05-17T07:30:17.372769Z","shell.execute_reply":"2024-05-17T07:30:17.371901Z","shell.execute_reply.started":"2024-05-17T07:30:17.367177Z"},"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["device"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T07:30:20.071829Z","iopub.status.busy":"2024-05-17T07:30:20.071467Z","iopub.status.idle":"2024-05-17T07:30:20.084331Z","shell.execute_reply":"2024-05-17T07:30:20.083242Z","shell.execute_reply.started":"2024-05-17T07:30:20.071799Z"},"trusted":true},"outputs":[],"source":["class TextTokenizer(torch.nn.Module):\n","    def __init__(\n","        self,\n","        text_tokenizer=BertTokenizer,\n","        max_length=25  # Add a max_length parameter\n","    ):\n","        super().__init__()\n","        self.text_tokenizer = text_tokenizer.from_pretrained('bert-base-uncased')\n","        self.max_length = max_length  # Store the max_length\n","\n","    def forward(self, input_question, padding='max_length', truncation=True):\n","        tokens = self.text_tokenizer(input_question, return_tensors='pt', \n","                                     padding=padding, truncation=truncation, \n","                                     max_length=self.max_length).to(device)  # Use max_length\n","\n","        return tokens\n","\n","class ImageProcessor(torch.nn.Module):\n","    def __init__(\n","        self,\n","        image_model_processor=ViTImageProcessor\n","    ):\n","\n","        super().__init__()\n","        self.image_model_processor = image_model_processor.from_pretrained('google/vit-base-patch16-224-in21k')\n","\n","    def forward(self, image):\n","        image = self.image_model_processor(image, return_tensors='pt').to(device)\n","\n","        return image\n","\n","class TextEmbedding(torch.nn.Module):\n","    def __init__(\n","        self,\n","        text_model=BertModel,\n","    ):\n","        super().__init__()\n","        self.text_model = text_model.from_pretrained('bert-base-uncased').to(device)\n","\n","\n","    def forward(self, tokens):\n","        text_output = self.text_model(input_ids=tokens.input_ids, attention_mask=tokens.attention_mask)\n","        text_output = text_output.last_hidden_state     # CLS token from the last layer\n","\n","        return text_output\n","\n","\n","class ImageEmbedding(torch.nn.Module):\n","    def __init__(\n","            self, \n","            image_model=ViTModel\n","        ):\n","        \n","        super().__init__()\n","        self.image_model = image_model.from_pretrained('google/vit-base-patch16-224-in21k').to(device)\n","\n","\n","    def forward(self, image):\n","        image_output = self.image_model(pixel_values=image.pixel_values).last_hidden_state\n","\n","        return image_output"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T07:30:22.734795Z","iopub.status.busy":"2024-05-17T07:30:22.734447Z","iopub.status.idle":"2024-05-17T07:30:22.743331Z","shell.execute_reply":"2024-05-17T07:30:22.742400Z","shell.execute_reply.started":"2024-05-17T07:30:22.734766Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'NotebookApp': {'iopub_msg_rate_limit': 10000, 'rate_limit_window': 10.0}}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["from notebook.services.config import ConfigManager\n","cm = ConfigManager()\n","cm.update('notebook', {\n","    'NotebookApp': {\n","        'iopub_msg_rate_limit': 10000,\n","        'rate_limit_window': 10.0\n","    }\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T07:30:24.767491Z","iopub.status.busy":"2024-05-17T07:30:24.766770Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for HuggingFaceM4/VQAv2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/HuggingFaceM4/VQAv2\n","You can avoid this message in future by passing the argument `trust_remote_code=True`.\n","Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n","  warnings.warn(\n","Repo card metadata block was not found. Setting CardData to empty.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"17356f281bb9405aa858a1c579cf81d0","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/13.5G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"HuggingFaceM4/VQAv2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataset = dataset['train']\n","test_dataset = dataset['test']\n","val_dataset = dataset['validation']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","train_df = pd.read_csv('/kaggle/input/vqdata/vqa_train_dataset.csv')\n","val_df = pd.read_csv('/kaggle/input/vqdata/vqa_val_dataset.csv')\n","\n","train_df = train_df[~train_df['answers'].isna()]\n","val_df = val_df[~val_df['answers'].isna()]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor\n","\n","class VQADataset(Dataset):\n","    def __init__(self, dataframe, image_dataset):\n","        self.dataframe = dataframe\n","        self.image_dataset = image_dataset\n","        self.text_tokenizer = TextTokenizer()\n","        self.image_processor = ImageProcessor()\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        row = self.dataframe.iloc[idx]\n","        \n","        ind = int(row['index'])\n","        image = self.image_dataset[ind]['image']\n","        question = row['question']\n","        answer = row['answers']\n","        \n","        # sanity check        \n","        assert self.image_dataset[ind]['question'] == question, \"Mismatching training and Image data\"\n","\n","        \n","        image = image.convert('RGB')\n","        \n","#         Preprocessing done in ImageEmbedder\n","#         preprocess = Compose([\n","#             Resize((224, 224)),\n","#             CenterCrop(224),\n","#             ToTensor(),\n","#         ])\n","#         image = preprocess(image)\n","\n","        # Tokenize question\n","        tokens = self.text_tokenizer(question, padding='max_length', truncation=True)\n","        tokens.input_ids = tokens.input_ids.squeeze()\n","        tokens.attention_mask = tokens.attention_mask.squeeze()\n","        image = self.image_processor(image)\n","        return {\n","            'image': image,\n","            'questions': tokens,\n","            'answer': answer\n","        }\n","\n","batch_size = 64\n","    \n","# Assuming you have separate dataframes for training and validation\n","train_data = VQADataset(train_df, train_dataset)\n","val_data = VQADataset(val_df, train_dataset)\n","\n","# DataLoader for training and validation\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pickle\n","\n","with open('/kaggle/input/vqadataset/answers_dictionaries_final.pkl', 'rb') as f:\n","    data = pickle.load(f)\n","    id_to_answer = data['id_to_answer']\n","    answer_to_id = data['answer_to_id']\n","\n","# print(\"Dictionaries have been loaded from answers_dictionaries.pkl\")\n","# print(\"ID to Answer Dictionary:\", id_to_answer)\n","# print(\"Answer dede bhai: \", answer_to_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class VQAModel(nn.Module):\n","    def __init__(\n","        self,\n","        dim_model = 768,      # image and text embeddings concatenated\n","        nhead = 12,                    # No. of Attention heads\n","        num_layers = 1,               # No. of encoder layers\n","        num_classes = 8161\n","    ):\n","        super().__init__()\n","        self.text_embedder = TextEmbedding()\n","        self.image_embedder = ImageEmbedding()\n","\n","        encoder_layers = TransformerEncoderLayer(d_model=dim_model, nhead=nhead)\n","        self.transformerEncoder = TransformerEncoder(encoder_layer=encoder_layers, num_layers=num_layers).to(device)\n","\n","        self.classifier = nn.Linear(dim_model, num_classes).to(device)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, questions, images):\n","        question_embedding = self.text_embedder(questions)\n","        image_embedding = self.image_embedder(images)\n","\n","        embeddings = torch.cat((question_embedding, image_embedding), dim=1)\n","        embeddings = embeddings.permute(1, 0, 2)  # (seq, batch, feature)\n","        output = self.transformerEncoder(embeddings)\n","\n","        cls_output = output[0, :, :]\n","\n","        logits = self.classifier(cls_output)\n","        output = self.softmax(logits)\n","\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = VQAModel(num_classes=len(answer_to_id))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# print([(n, type(m)) for n, m in model().named_modules()])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from peft import LoraConfig, get_peft_model\n","\n","# Define the LoRA configuration\n","LORA_R = 4\n","LORA_ALPHA = 512\n","LORA_DROPOUT = 0.05\n","\n","lora_config = LoraConfig(\n","    r=LORA_R,\n","    lora_alpha=LORA_ALPHA,\n","    lora_dropout=LORA_DROPOUT,\n","    bias=\"none\",\n","    target_modules=[\n","        \"transformerEncoder.layers.0.linear1\",\n","        \"transformerEncoder.layers.0.linear2\",\n","        \"image_embedder.image_model.encoder.layer.11.intermediate.dense\",\n","        \"image_embedder.image_model.encoder.layer.11.output.dense\"\n","    ]\n",")\n","\n","# Initialize the VQALORAModel\n","# model = VQALORAModel()\n","\n","# Apply LoRA to the model\n","lora_model = get_peft_model(model, lora_config)\n","\n","# Print the trainable parameters\n","lora_model.print_trainable_parameters()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.003)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from torch.utils.tensorboard import SummaryWriter\n","\n","# Create a SummaryWriter object\n","writer = SummaryWriter('runs/experiment_lora_4')\n","num_epochs = 3\n","lora_model.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["checkpoint_ref = 100"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from torchmetrics.classification import Precision, Recall, Accuracy, F1Score, AUROC\n","\n","precision_metric = Precision(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n","recall_metric = Recall(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n","accuracy_metric = Accuracy(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n","f1_metric = F1Score(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n","# auroc_metric = AUROC(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n","\n","def evaluate(preds, true):\n","    p = precision_metric(preds, true)\n","    r = recall_metric(preds, true)\n","    a = accuracy_metric(preds, true)\n","    f = f1_metric(preds, true)\n","#     am = auroc_metric(preds, true)\n","    \n","    return {\n","        \"precision\": p,\n","        \"recall\": r,\n","        \"accuracy\": a,\n","        \"f1\": f,\n","#         \"auroc\": auroc_metric\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!mkdir ./checkpoints"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch.nn.functional as F\n","\n","batch_no = 0\n","avg_accuracy = 0\n","\n","for epoch in range(num_epochs):\n","    batch_no = 0\n","    avg_accuracy = 0\n","    for batch in train_dataloader:\n","        # Get the inputs and targets from the batch\n","        images = batch['image']\n","        questions = batch['questions']\n","        answers = batch['answer']\n","\n","        questions.input_ids = questions.input_ids.squeeze()\n","        questions.attention_mask = questions.attention_mask.squeeze()\n","        images.pixel_values = images.pixel_values.squeeze()\n","        \n","        # Forward pass\n","        outputs = lora_model(questions, images)\n","\n","        \n","        answers_ids = [answer_to_id[key] for key in answers]\n","        answers_ids_tensor = torch.tensor(answers_ids)\n","        answers = F.one_hot(answers_ids_tensor, num_classes=len(answer_to_id)).to(outputs.dtype)\n","        answers = answers.to(device)\n","    \n","        \n","        # Sanity check\n","        assert answers.shape == outputs.shape, \"Target and Predicted shapes don't match\"\n","    \n","        # Compute the loss\n","        loss = criterion(outputs, answers)\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        \n","        _, preds = torch.max(outputs, 1)\n","        _, answer_inds = torch.max(answers, 1)\n","        \n","        assert preds.shape == answer_inds.shape, f\"Preds_shape: {preds.shape}, Answers_shape: {answer_inds.shape}\"\n","        eval_met = evaluate(preds, answer_inds)\n","        \n","        iter_val = epoch * len(train_dataloader) + batch_no\n","        \n","        \n","        writer.add_scalar('Training Loss', loss.item(), iter_val)\n","        writer.add_pr_curve('PR Curve', answers, outputs, iter_val)\n","        writer.add_scalar('Accuracy', eval_met['accuracy'], iter_val)\n","        writer.add_scalar('Precision', eval_met['precision'], iter_val)\n","        writer.add_scalar('Recall', eval_met['recall'], iter_val)\n","        writer.add_scalar('F1', eval_met['f1'], iter_val)\n","         \n","        \n","        if batch_no % checkpoint_ref == 0:\n","            torch.save(model.state_dict(), f\"./checkpoints/lora_4.pth\")\n","            \n","        batch_no += 1\n","        print(f\"Batch -> {batch_no} done -> Accu: {eval_met['accuracy']:.2f} -> Prec: {eval_met['precision']:.2f}\\r\", end=\"\")\n","        avg_accuracy += eval_met['accuracy']\n","        \n","\n","    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Avg. Epoch Acc: {avg_accuracy / len(train_dataloader)}')\n","# writer.close()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5020279,"sourceId":8430164,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
