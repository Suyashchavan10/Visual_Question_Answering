{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Training Coattention Model"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T12:08:40.644778Z","iopub.status.busy":"2024-05-18T12:08:40.644359Z","iopub.status.idle":"2024-05-18T12:08:49.642254Z","shell.execute_reply":"2024-05-18T12:08:49.641425Z","shell.execute_reply.started":"2024-05-18T12:08:40.644736Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-18 12:08:45.998640: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-18 12:08:45.998709: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-18 12:08:46.002718: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["from transformers import BertModel, BertTokenizer, ViTImageProcessor, ViTModel\n","import torch\n","from torchinfo import summary\n","from torch import nn\n","from torch.nn import Transformer, TransformerDecoder, TransformerDecoderLayer, TransformerEncoder, TransformerEncoderLayer"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T12:08:49.644454Z","iopub.status.busy":"2024-05-18T12:08:49.643878Z","iopub.status.idle":"2024-05-18T12:08:49.680099Z","shell.execute_reply":"2024-05-18T12:08:49.679203Z","shell.execute_reply.started":"2024-05-18T12:08:49.644415Z"},"trusted":true},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# device = torch.device('cpu')"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T12:08:49.682648Z","iopub.status.busy":"2024-05-18T12:08:49.682168Z","iopub.status.idle":"2024-05-18T12:08:49.698097Z","shell.execute_reply":"2024-05-18T12:08:49.697274Z","shell.execute_reply.started":"2024-05-18T12:08:49.682596Z"},"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["device"]},{"cell_type":"markdown","metadata":{},"source":["## Initialize Necessary Modules"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T12:08:49.700815Z","iopub.status.busy":"2024-05-18T12:08:49.700554Z","iopub.status.idle":"2024-05-18T12:08:49.712609Z","shell.execute_reply":"2024-05-18T12:08:49.711716Z","shell.execute_reply.started":"2024-05-18T12:08:49.700793Z"},"trusted":true},"outputs":[],"source":["class TextTokenizer(torch.nn.Module):\n","    def __init__(\n","        self,\n","        text_tokenizer=BertTokenizer,\n","        max_length=25  # Add a max_length parameter\n","    ):\n","        super().__init__()\n","        self.text_tokenizer = text_tokenizer.from_pretrained('bert-base-uncased')\n","        self.max_length = max_length  # Store the max_length\n","\n","    def forward(self, input_question, padding='max_length', truncation=True):\n","        tokens = self.text_tokenizer(input_question, return_tensors='pt', \n","                                     padding=padding, truncation=truncation, \n","                                     max_length=self.max_length).to(device)  # Use max_length\n","\n","        return tokens\n","\n","class ImageProcessor(torch.nn.Module):\n","    def __init__(\n","        self,\n","        image_model_processor=ViTImageProcessor\n","    ):\n","\n","        super().__init__()\n","        self.image_model_processor = image_model_processor.from_pretrained('google/vit-base-patch16-224-in21k')\n","\n","    def forward(self, image):\n","        image = self.image_model_processor(image, return_tensors='pt').to(device)\n","\n","        return image\n","\n","class TextEmbedding(torch.nn.Module):\n","    def __init__(\n","        self,\n","        text_model=BertModel,\n","    ):\n","        super().__init__()\n","        self.text_model = text_model.from_pretrained('bert-base-uncased').to(device)\n","\n","\n","    def forward(self, tokens):\n","        text_output = self.text_model(input_ids=tokens.input_ids, attention_mask=tokens.attention_mask)\n","        text_output = text_output.last_hidden_state     # CLS token from the last layer\n","\n","        return text_output\n","\n","\n","class ImageEmbedding(torch.nn.Module):\n","    def __init__(\n","            self, \n","            image_model=ViTModel\n","        ):\n","        \n","        super().__init__()\n","        self.image_model = image_model.from_pretrained('google/vit-base-patch16-224-in21k').to(device)\n","\n","\n","    def forward(self, image):\n","        image_output = self.image_model(pixel_values=image.pixel_values).last_hidden_state\n","\n","        return image_output"]},{"cell_type":"markdown","metadata":{},"source":["## Load Dataset"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T12:08:49.764404Z","iopub.status.busy":"2024-05-18T12:08:49.764100Z","iopub.status.idle":"2024-05-18T12:08:51.825591Z","shell.execute_reply":"2024-05-18T12:08:51.824650Z","shell.execute_reply.started":"2024-05-18T12:08:49.764381Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for HuggingFaceM4/VQAv2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/HuggingFaceM4/VQAv2\n","You can avoid this message in future by passing the argument `trust_remote_code=True`.\n","Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n","  warnings.warn(\n","Repo card metadata block was not found. Setting CardData to empty.\n"]}],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"HuggingFaceM4/VQAv2\")"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T12:08:51.827600Z","iopub.status.busy":"2024-05-18T12:08:51.827045Z","iopub.status.idle":"2024-05-18T12:08:51.831878Z","shell.execute_reply":"2024-05-18T12:08:51.830873Z","shell.execute_reply.started":"2024-05-18T12:08:51.827572Z"},"trusted":true},"outputs":[],"source":["train_dataset = dataset['train']\n","test_dataset = dataset['test']\n","val_dataset = dataset['validation']"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T12:08:51.833471Z","iopub.status.busy":"2024-05-18T12:08:51.833122Z","iopub.status.idle":"2024-05-18T12:08:52.190353Z","shell.execute_reply":"2024-05-18T12:08:52.189529Z","shell.execute_reply.started":"2024-05-18T12:08:51.833427Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","train_df = pd.read_csv('/kaggle/input/vqdata/vqa_train_dataset.csv')\n","val_df = pd.read_csv('/kaggle/input/vqdata/vqa_val_dataset.csv')\n","\n","train_df = train_df[~train_df['answers'].isna()]\n","val_df = val_df[~val_df['answers'].isna()]"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T12:08:52.191989Z","iopub.status.busy":"2024-05-18T12:08:52.191707Z","iopub.status.idle":"2024-05-18T12:08:52.214595Z","shell.execute_reply":"2024-05-18T12:08:52.213605Z","shell.execute_reply.started":"2024-05-18T12:08:52.191965Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>image_name</th>\n","      <th>question</th>\n","      <th>answers</th>\n","      <th>question_type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["Empty DataFrame\n","Columns: [index, image_name, question, answers, question_type]\n","Index: []"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["train_df[train_df['answers'].isna()]"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T12:08:52.216532Z","iopub.status.busy":"2024-05-18T12:08:52.216220Z","iopub.status.idle":"2024-05-18T12:08:52.991130Z","shell.execute_reply":"2024-05-18T12:08:52.990051Z","shell.execute_reply.started":"2024-05-18T12:08:52.216505Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor\n","\n","class VQADataset(Dataset):\n","    def __init__(self, dataframe, image_dataset):\n","        self.dataframe = dataframe\n","        self.image_dataset = image_dataset\n","        self.text_tokenizer = TextTokenizer()\n","        self.image_processor = ImageProcessor()\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        row = self.dataframe.iloc[idx]\n","        \n","        ind = int(row['index'])\n","        image = self.image_dataset[ind]['image']\n","        question = row['question']\n","        answer = row['answers']\n","        \n","        # sanity check        \n","        assert self.image_dataset[ind]['question'] == question, \"Mismatching training and Image data\"\n","\n","        \n","        image = image.convert('RGB')\n","\n","        tokens = self.text_tokenizer(question, padding='max_length', truncation=True)\n","        tokens.input_ids = tokens.input_ids.squeeze()\n","        tokens.attention_mask = tokens.attention_mask.squeeze()\n","        \n","        answer_tokens = self.text_tokenizer(answer, padding='max_length', truncation=True)\n","        answer_tokens.input_ids = answer_tokens.input_ids.squeeze()\n","        answer_tokens.attention_mask = tokens.attention_mask.squeeze()\n","        \n","        \n","        image = self.image_processor(image)\n","        return {\n","            'image': image,\n","            'questions': tokens,\n","            'answer': answer,\n","            'answer_tokens' : answer_tokens\n","        }\n","\n","batch_size = 64\n","    \n","# Assuming you have separate dataframes for training and validation\n","train_data = VQADataset(train_df, train_dataset)\n","val_data = VQADataset(val_df, train_dataset)\n","\n","# DataLoader for training and validation\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T12:08:52.992778Z","iopub.status.busy":"2024-05-18T12:08:52.992475Z","iopub.status.idle":"2024-05-18T12:08:53.003671Z","shell.execute_reply":"2024-05-18T12:08:53.002889Z","shell.execute_reply.started":"2024-05-18T12:08:52.992752Z"},"trusted":true},"outputs":[],"source":["import pickle\n","\n","with open('/kaggle/input/vqdata/answers_dictionaries.pkl', 'rb') as f:\n","    data = pickle.load(f)\n","    id_to_answer = data['id_to_answer']\n","    answer_to_id = data['answer_to_id']\n","\n","# print(\"Dictionaries have been loaded from answers_dictionaries.pkl\")\n","# print(\"ID to Answer Dictionary:\", id_to_answer)\n","# print(\"Answer dede bhai: \", answer_to_id)"]},{"cell_type":"markdown","metadata":{},"source":["## Model Creation"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T12:08:53.005225Z","iopub.status.busy":"2024-05-18T12:08:53.004915Z","iopub.status.idle":"2024-05-18T12:08:53.013752Z","shell.execute_reply":"2024-05-18T12:08:53.012824Z","shell.execute_reply.started":"2024-05-18T12:08:53.005185Z"},"trusted":true},"outputs":[],"source":["class VQAModel(nn.Module):\n","    def __init__(\n","        self,\n","        dim_model = 768,      # image and text embeddings concatenated\n","        nhead = 12,                    # No. of Attention heads\n","        num_layers = 1,               # No. of encoder layers\n","        num_classes = 8000\n","    ):\n","        super().__init__()\n","        self.text_embedder = TextEmbedding()\n","        self.image_embedder = ImageEmbedding()\n","        \n","        self.transformer = Transformer(num_encoder_layers=1, num_decoder_layers=1, nhead=nhead, d_model=dim_model).to(device)\n","\n","\n","    def forward(self, questions, images, answers):\n","        question_embedding = self.text_embedder(questions)\n","        image_embedding = self.image_embedder(images)\n","        answer_embedding = self.text_embedder(answers)\n","        \n","        embeddings = torch.cat((question_embedding, image_embedding), dim=1)\n","        embeddings = embeddings.permute(1, 0, 2)  # (seq, batch, feature)\n","        answer_embedding = answer_embedding.permute(1, 0, 2)\n","#         print(embeddings.shape, answer_embedding.shape)\n","        output = self.transformer(embeddings, answer_embedding)\n","\n","        return output[:answer_embedding.shape[0],:,:], answer_embedding"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T12:08:53.015671Z","iopub.status.busy":"2024-05-18T12:08:53.015050Z","iopub.status.idle":"2024-05-18T12:08:54.560877Z","shell.execute_reply":"2024-05-18T12:08:54.559861Z","shell.execute_reply.started":"2024-05-18T12:08:53.015639Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"]}],"source":["assert len(answer_to_id) == len(id_to_answer)\n","model = VQAModel(num_classes=len(answer_to_id))"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T12:08:54.562974Z","iopub.status.busy":"2024-05-18T12:08:54.562320Z","iopub.status.idle":"2024-05-18T12:08:54.584665Z","shell.execute_reply":"2024-05-18T12:08:54.583650Z","shell.execute_reply.started":"2024-05-18T12:08:54.562938Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>image_name</th>\n","      <th>question</th>\n","      <th>answers</th>\n","      <th>question_type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["Empty DataFrame\n","Columns: [index, image_name, question, answers, question_type]\n","Index: []"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["train_df[train_df['answers'].isna()]"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T12:14:23.361981Z","iopub.status.busy":"2024-05-18T12:14:23.361265Z","iopub.status.idle":"2024-05-18T12:14:37.021917Z","shell.execute_reply":"2024-05-18T12:14:37.020746Z","shell.execute_reply.started":"2024-05-18T12:14:23.361949Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting peft\n","  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\n","Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\n","Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.39.3)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\n","Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.29.3)\n","Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\n","Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.22.2)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.2.0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: peft\n","Successfully installed peft-0.11.1\n"]}],"source":["!pip install peft"]},{"cell_type":"markdown","metadata":{},"source":["### Comment the next cell and uncomment the next of next cell to run Encoder-decoder VQA Model without LoRA"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T13:10:24.425482Z","iopub.status.busy":"2024-05-18T13:10:24.425100Z","iopub.status.idle":"2024-05-18T13:10:24.453525Z","shell.execute_reply":"2024-05-18T13:10:24.452595Z","shell.execute_reply.started":"2024-05-18T13:10:24.425447Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 122,880 || all params: 209,389,312 || trainable%: 0.0587\n"]}],"source":["# Apply LoRA\n","\n","from peft import LoraConfig, get_peft_model\n","\n","# Define the LoRA configuration\n","LORA_R = 16\n","LORA_ALPHA = 512\n","LORA_DROPOUT = 0.05\n","\n","lora_config = LoraConfig(\n","    r=LORA_R,\n","    lora_alpha=LORA_ALPHA,\n","    lora_dropout=LORA_DROPOUT,\n","    bias=\"none\",\n","    target_modules=[\n","        \"transformerEncoder.layers.0.linear1\",\n","        \"transformerEncoder.layers.0.linear2\",\n","        \"image_embedder.image_model.encoder.layer.11.intermediate.dense\",\n","        \"image_embedder.image_model.encoder.layer.11.output.dense\"\n","    ]\n","#     target_modules = linear_layers\n",")\n","\n","# Initialize the VQALORAModel\n","# model = VQALORAModel()\n","\n","# Apply LoRA to the model\n","model = get_peft_model(model, lora_config)\n","\n","# Print the trainable parameters\n","model.print_trainable_parameters()\n","model.train()\n","writer = SummaryWriter('runs/experiment_self_supervised_lora_16')\n","num_epochs = 3"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T13:10:24.455276Z","iopub.status.busy":"2024-05-18T13:10:24.455004Z","iopub.status.idle":"2024-05-18T13:10:24.458991Z","shell.execute_reply":"2024-05-18T13:10:24.458180Z","shell.execute_reply.started":"2024-05-18T13:10:24.455252Z"},"trusted":true},"outputs":[],"source":["# from torch.utils.tensorboard import SummaryWriter\n","\n","# # Create a SummaryWriter object\n","# writer = SummaryWriter('runs/experiment_self_supervised')\n","# num_epochs = 3\n","# model.train()"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T13:10:24.460493Z","iopub.status.busy":"2024-05-18T13:10:24.460206Z","iopub.status.idle":"2024-05-18T13:10:24.472249Z","shell.execute_reply":"2024-05-18T13:10:24.471386Z","shell.execute_reply.started":"2024-05-18T13:10:24.460457Z"},"trusted":true},"outputs":[],"source":["criterion = nn.CosineEmbeddingLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n","# loss = criterion(out, tar)\n","# loss.backward()\n","# optimizer.step()"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T13:10:24.475090Z","iopub.status.busy":"2024-05-18T13:10:24.474615Z","iopub.status.idle":"2024-05-18T13:10:24.481180Z","shell.execute_reply":"2024-05-18T13:10:24.480405Z","shell.execute_reply.started":"2024-05-18T13:10:24.475056Z"},"trusted":true},"outputs":[],"source":["checkpoint_ref = 100"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation Metrics"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T13:10:24.482509Z","iopub.status.busy":"2024-05-18T13:10:24.482173Z","iopub.status.idle":"2024-05-18T13:10:24.489816Z","shell.execute_reply":"2024-05-18T13:10:24.488969Z","shell.execute_reply.started":"2024-05-18T13:10:24.482474Z"},"trusted":true},"outputs":[],"source":["from torchmetrics.classification import Precision, Recall, Accuracy, F1Score, AUROC"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T13:10:24.492888Z","iopub.status.busy":"2024-05-18T13:10:24.492605Z","iopub.status.idle":"2024-05-18T13:10:24.504932Z","shell.execute_reply":"2024-05-18T13:10:24.504120Z","shell.execute_reply.started":"2024-05-18T13:10:24.492864Z"},"trusted":true},"outputs":[],"source":["precision_metric = Precision(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n","recall_metric = Recall(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n","accuracy_metric = Accuracy(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n","f1_metric = F1Score(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n","# auroc_metric = AUROC(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n","\n","def evaluate(preds, true):\n","    p = precision_metric(preds, true)\n","    r = recall_metric(preds, true)\n","    a = accuracy_metric(preds, true)\n","    f = f1_metric(preds, true)\n","#     am = auroc_metric(preds, true)\n","    \n","    return {\n","        \"precision\": p,\n","        \"recall\": r,\n","        \"accuracy\": a,\n","        \"f1\": f,\n","#         \"auroc\": auroc_metric\n","    }\n"]},{"cell_type":"markdown","metadata":{},"source":["## Training Loop"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T13:10:24.506585Z","iopub.status.busy":"2024-05-18T13:10:24.506003Z","iopub.status.idle":"2024-05-18T13:10:24.512508Z","shell.execute_reply":"2024-05-18T13:10:24.511675Z","shell.execute_reply.started":"2024-05-18T13:10:24.506552Z"},"trusted":true},"outputs":[],"source":["# !mkdir ./checkpoints"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T13:10:24.668057Z","iopub.status.busy":"2024-05-18T13:10:24.667321Z","iopub.status.idle":"2024-05-18T14:37:14.276096Z","shell.execute_reply":"2024-05-18T14:37:14.273957Z","shell.execute_reply.started":"2024-05-18T13:10:24.668024Z"},"trusted":true},"outputs":[],"source":["import torch.nn.functional as F\n","\n","batch_no = 0\n","avg_accuracy = 0\n","\n","for epoch in range(num_epochs):\n","    batch_no = 0\n","    avg_accuracy = 0\n","    for batch in train_dataloader:\n","        # Get the inputs and targets from the batch\n","        images = batch['image']\n","        questions = batch['questions']\n","        answers = batch['answer_tokens']\n","\n","        questions.input_ids = questions.input_ids.squeeze()\n","        questions.attention_mask = questions.attention_mask.squeeze()\n","        images.pixel_values = images.pixel_values.squeeze()\n","        answers.input_ids = answers.input_ids.squeeze()\n","        answers.attention_mask = answers.attention_mask.squeeze()\n","    \n","        # Forward pass\n","        outputs, answers = model(questions, images, answers)\n","        \n","        target = torch.ones(outputs.shape[1] * outputs.shape[0]).to(device)\n","        \n","        outputs = outputs.reshape(-1, 768)\n","        answers = answers.reshape(-1, 768)\n","        \n","        loss = criterion(outputs, answers, target)\n"," \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        iter_val = epoch * len(train_dataloader) + batch_no\n","        \n","        \n","        \n","        writer.add_scalar('Training Loss', loss.item(), iter_val)\n","        \n","        \n","        if batch_no % checkpoint_ref == 0:\n","            torch.save(model.state_dict(), f\"./checkpoints/latest.pth\")\n","            \n","        batch_no += 1\n","        print(f\"Batch -> {batch_no} done -> cosine Loss: {loss}\\r\", end=\"\")\n","#         avg_accuracy += eval_met['accuracy']\n","        \n","\n","    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n","# writer.close()"]},{"cell_type":"markdown","metadata":{},"source":["### Model Evaluation"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T14:37:20.495244Z","iopub.status.busy":"2024-05-18T14:37:20.494874Z","iopub.status.idle":"2024-05-18T14:37:20.502536Z","shell.execute_reply":"2024-05-18T14:37:20.501484Z","shell.execute_reply.started":"2024-05-18T14:37:20.495214Z"},"trusted":true},"outputs":[],"source":["def pearson_correlation(x, y):\n","    mean_x = torch.mean(x, dim=1, keepdim=True)\n","    mean_y = torch.mean(y, dim=1, keepdim=True)\n","    xm = x.sub(mean_x)\n","    ym = y.sub(mean_y)\n","    r_num = xm.mul(ym).sum(dim=1)\n","    r_den = torch.norm(xm, 2, dim=1) * torch.norm(ym, 2, dim=1)\n","    r_val = r_num / r_den\n","    return r_val.sum()"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T14:37:33.399072Z","iopub.status.busy":"2024-05-18T14:37:33.398359Z","iopub.status.idle":"2024-05-18T14:37:33.403985Z","shell.execute_reply":"2024-05-18T14:37:33.402904Z","shell.execute_reply.started":"2024-05-18T14:37:33.399041Z"},"trusted":true},"outputs":[],"source":["writer.close()"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T14:38:09.056089Z","iopub.status.busy":"2024-05-18T14:38:09.055151Z","iopub.status.idle":"2024-05-18T14:49:08.196778Z","shell.execute_reply":"2024-05-18T14:49:08.195581Z","shell.execute_reply.started":"2024-05-18T14:38:09.056058Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss: 216.48974961042404, Time taken: 658.4874546527863\n"]}],"source":["import torch.nn.functional as F\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.metrics.pairwise import pairwise_distances\n","from scipy.stats import pearsonr\n","import numpy as np\n","import time\n","model.eval()\n","batch_no = 0\n","\n","\n","with torch.no_grad():\n","    total_loss = 0\n","    total_cosine = 0\n","    total_euc = 0\n","    total_manhattan = 0\n","    total_pearson = 0\n","    start_time = time.time()\n","    for batch in val_dataloader:\n","        # Get the inputs and targets from the batch\n","        images = batch['image']\n","        questions = batch['questions']\n","        answers = batch['answer_tokens']\n","\n","        questions.input_ids = questions.input_ids.squeeze()\n","        questions.attention_mask = questions.attention_mask.squeeze()\n","        images.pixel_values = images.pixel_values.squeeze()\n","        answers.input_ids = answers.input_ids.squeeze()\n","        answers.attention_mask = answers.attention_mask.squeeze()\n","    \n","        # Forward pass\n","        outputs, answers = model(questions, images, answers)\n","\n","        target = torch.ones(outputs.shape[1] * outputs.shape[0]).to(device)\n","        \n","                \n","        outputs = outputs.reshape(-1, 768)\n","        answers = answers.reshape(-1, 768)\n","        \n","        loss = criterion(outputs, answers, target)\n","        total_loss += loss.item()\n","        \n","        total_pearson += pearson_correlation(outputs, answers)\n","        total_cosine += torch.sum(F.cosine_similarity(outputs, answers))\n","        \n","        euclidean_distances = torch.sqrt(torch.sum((outputs - answers) ** 2, dim=1))\n","        total_euc = torch.sum(euclidean_distances)\n","        \n","        manhattan_distances = torch.sum(torch.abs(outputs - answers), dim=1)\n","        total_manhattan = torch.sum(manhattan_distances)\n","\n","        \n","        print(f\"Batch -> {batch_no} done -> Loss: {loss.item()}\\r\", end=\"\")\n","        batch_no += 1\n","#         if batch_no == 1: break\n","        \n","    end_time = time.time()\n","    print(f'Loss: {total_loss}, Time taken: {end_time - start_time}')\n","# writer.close()"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T14:49:08.198824Z","iopub.status.busy":"2024-05-18T14:49:08.198514Z","iopub.status.idle":"2024-05-18T14:49:08.329909Z","shell.execute_reply":"2024-05-18T14:49:08.328895Z","shell.execute_reply.started":"2024-05-18T14:49:08.198797Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Average Correlation:  tensor(0.5385, device='cuda:0')\n","Average Euclidean Distance:  tensor(24556.4922, device='cuda:0') 750400\n","Average Manhattan Distance:  tensor(567850.6250, device='cuda:0') 750400\n","Average cosine similarity:  tensor(0.5380, device='cuda:0')\n","Total time taken:  658.4874546527863\n"]}],"source":["print(\"Average Correlation: \", total_pearson / (25 * 64 * len(val_dataloader)))\n","print(\"Average Euclidean Distance: \", total_euc, (25 * 64 * len(val_dataloader)))\n","print(\"Average Manhattan Distance: \", total_manhattan, (25 * 64 * len(val_dataloader)))\n","print(\"Average cosine similarity: \", total_cosine / (25 * 64 * len(val_dataloader)))\n","print(\"Total time taken: \", end_time - start_time)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5020279,"sourceId":8430164,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
