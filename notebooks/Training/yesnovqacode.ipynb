{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8429828,"sourceType":"datasetVersion","datasetId":5020027},{"sourceId":8436404,"sourceType":"datasetVersion","datasetId":5024941}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# only CLS token taken from both ViT and BERT","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertModel, BertTokenizer, ViTImageProcessor, ViTModel\nimport torch\nfrom torchinfo import summary\nfrom torch import nn\nfrom torch.nn import Transformer, TransformerDecoder, TransformerDecoderLayer, TransformerEncoder, TransformerEncoderLayer","metadata":{"execution":{"iopub.status.busy":"2024-05-17T09:00:40.289633Z","iopub.execute_input":"2024-05-17T09:00:40.290328Z","iopub.status.idle":"2024-05-17T09:00:40.295397Z","shell.execute_reply.started":"2024-05-17T09:00:40.290300Z","shell.execute_reply":"2024-05-17T09:00:40.294337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# device = torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2024-05-17T09:00:52.418350Z","iopub.execute_input":"2024-05-17T09:00:52.418721Z","iopub.status.idle":"2024-05-17T09:00:52.475930Z","shell.execute_reply.started":"2024-05-17T09:00:52.418693Z","shell.execute_reply":"2024-05-17T09:00:52.474883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2024-05-17T09:01:08.395905Z","iopub.execute_input":"2024-05-17T09:01:08.396616Z","iopub.status.idle":"2024-05-17T09:01:08.403132Z","shell.execute_reply.started":"2024-05-17T09:01:08.396583Z","shell.execute_reply":"2024-05-17T09:01:08.402063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextTokenizer(torch.nn.Module):\n    def __init__(\n        self,\n        text_tokenizer=BertTokenizer,\n        max_length=25  # Add a max_length parameter\n    ):\n        super().__init__()\n        self.text_tokenizer = text_tokenizer.from_pretrained('bert-base-uncased')\n        self.max_length = max_length  # Store the max_length\n\n    def forward(self, input_question, padding='max_length', truncation=True):\n        tokens = self.text_tokenizer(input_question, return_tensors='pt', \n                                     padding=padding, truncation=truncation, \n                                     max_length=self.max_length).to(device)  # Use max_length\n\n        return tokens\n\nclass ImageProcessor(torch.nn.Module):\n    def __init__(\n        self,\n        image_model_processor=ViTImageProcessor\n    ):\n\n        super().__init__()\n        self.image_model_processor = image_model_processor.from_pretrained('google/vit-base-patch16-224-in21k')\n\n    def forward(self, image):\n        image = self.image_model_processor(image, return_tensors='pt').to(device)\n\n        return image\n\nclass TextEmbedding(torch.nn.Module):\n    def __init__(\n        self,\n        text_model=BertModel,\n    ):\n        super().__init__()\n        self.text_model = text_model.from_pretrained('bert-base-uncased').to(device)\n\n\n    def forward(self, tokens):\n        text_output = self.text_model(input_ids=tokens.input_ids, attention_mask=tokens.attention_mask)\n        text_output = text_output.last_hidden_state     # CLS token from the last layer\n\n        return text_output\n\n\nclass ImageEmbedding(torch.nn.Module):\n    def __init__(\n            self, \n            image_model=ViTModel\n        ):\n        \n        super().__init__()\n        self.image_model = image_model.from_pretrained('google/vit-base-patch16-224-in21k').to(device)\n\n\n    def forward(self, image):\n        image_output = self.image_model(pixel_values=image.pixel_values).last_hidden_state\n\n        return image_output","metadata":{"execution":{"iopub.status.busy":"2024-05-17T09:01:14.266652Z","iopub.execute_input":"2024-05-17T09:01:14.266979Z","iopub.status.idle":"2024-05-17T09:01:14.278656Z","shell.execute_reply.started":"2024-05-17T09:01:14.266954Z","shell.execute_reply":"2024-05-17T09:01:14.277640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# textEmbedder = TextEmbedding()\n# imageEmbedder = ImageEmbedding()\n\n# sentence = \"Hello, I am paris\"\n# img = torch.rand((3, 224, 224))\n\n# text_emd = textEmbedder(sentence)\n# image_emd = imageEmbedder(img)\n\n# print(text_emd.shape)\n# print(image_emd.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T09:01:20.717426Z","iopub.execute_input":"2024-05-17T09:01:20.718115Z","iopub.status.idle":"2024-05-17T09:01:20.721896Z","shell.execute_reply.started":"2024-05-17T09:01:20.718064Z","shell.execute_reply":"2024-05-17T09:01:20.721005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VQAModel(nn.Module):\n    def __init__(\n        self,\n        dim_model = 768,      # image and text embeddings concatenated\n        nhead = 12,                    # No. of Attention heads\n        num_layers = 1,               # No. of encoder layers\n        num_classes = 2\n    ):\n        super().__init__()\n        self.text_embedder = TextEmbedding()\n        self.image_embedder = ImageEmbedding()\n\n        encoder_layers = TransformerEncoderLayer(d_model=dim_model, nhead=nhead)\n        self.transformerEncoder = TransformerEncoder(encoder_layer=encoder_layers, num_layers=num_layers).to(device)\n\n        self.classifier = nn.Linear(dim_model, num_classes).to(device)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, questions, images):\n        question_embedding = self.text_embedder(questions)\n        image_embedding = self.image_embedder(images)\n\n        embeddings = torch.cat((question_embedding, image_embedding), dim=1)\n        embeddings = embeddings.permute(1, 0, 2)  # (seq, batch, feature)\n        output = self.transformerEncoder(embeddings)\n\n        cls_output = output[0, :, :]\n\n        logits = self.classifier(cls_output)\n        output = self.softmax(logits)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-05-17T09:01:23.901843Z","iopub.execute_input":"2024-05-17T09:01:23.902966Z","iopub.status.idle":"2024-05-17T09:01:23.911600Z","shell.execute_reply.started":"2024-05-17T09:01:23.902924Z","shell.execute_reply":"2024-05-17T09:01:23.910691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# question = \"How are you?\"\n# img = torch.rand(3, 224, 224)\n# target = \"I am OK, how about you\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# out = model([question], [img])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tar = torch.zeros(1, 8000).to(device)\n# tar[0, 4] = 1\n# tar.shape, out.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# criterion = nn.CrossEntropyLoss()\n# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n# loss = criterion(out, tar)\n# loss.backward()\n# optimizer.step()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pip install --upgrade transformers datasets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pip install transformers datasets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"HuggingFaceM4/VQAv2\")","metadata":{"execution":{"iopub.status.busy":"2024-05-17T09:01:40.048695Z","iopub.execute_input":"2024-05-17T09:01:40.049386Z","iopub.status.idle":"2024-05-17T09:20:24.690974Z","shell.execute_reply.started":"2024-05-17T09:01:40.049355Z","shell.execute_reply":"2024-05-17T09:20:24.689983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = dataset['train']\ntest_dataset = dataset['test']\nval_dataset = dataset['validation']","metadata":{"execution":{"iopub.status.busy":"2024-05-17T09:20:42.866970Z","iopub.execute_input":"2024-05-17T09:20:42.867810Z","iopub.status.idle":"2024-05-17T09:20:42.872272Z","shell.execute_reply.started":"2024-05-17T09:20:42.867775Z","shell.execute_reply":"2024-05-17T09:20:42.871333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ntrain_df = pd.read_csv('/kaggle/input/vqayesno/small_train_dataset_final.csv')\nval_df = pd.read_csv('/kaggle/input/vqayesno/small_val_dataset_final.csv')\n\ntrain_df = train_df[~train_df['answers'].isna()]\nval_df = val_df[~val_df['answers'].isna()]","metadata":{"execution":{"iopub.status.busy":"2024-05-17T09:20:44.809178Z","iopub.execute_input":"2024-05-17T09:20:44.809837Z","iopub.status.idle":"2024-05-17T09:20:45.199160Z","shell.execute_reply.started":"2024-05-17T09:20:44.809809Z","shell.execute_reply":"2024-05-17T09:20:45.198243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[train_df['answers'].isna()]","metadata":{"execution":{"iopub.status.busy":"2024-05-17T09:20:46.052498Z","iopub.execute_input":"2024-05-17T09:20:46.052846Z","iopub.status.idle":"2024-05-17T09:20:46.080158Z","shell.execute_reply.started":"2024-05-17T09:20:46.052820Z","shell.execute_reply":"2024-05-17T09:20:46.079136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom torchvision.transforms import Compose, Resize, CenterCrop, ToTensor\n\nclass VQADataset(Dataset):\n    def __init__(self, dataframe, image_dataset):\n        self.dataframe = dataframe\n        self.image_dataset = image_dataset\n        self.text_tokenizer = TextTokenizer()\n        self.image_processor = ImageProcessor()\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        \n        ind = int(row['index'])\n        image = self.image_dataset[ind]['image']\n        question = row['question']\n        answer = row['answers']\n        \n        # sanity check        \n        assert self.image_dataset[ind]['question'] == question, \"Mismatching training and Image data\"\n\n        \n        image = image.convert('RGB')\n        \n#         Preprocessing done in ImageEmbedder\n#         preprocess = Compose([\n#             Resize((224, 224)),\n#             CenterCrop(224),\n#             ToTensor(),\n#         ])\n#         image = preprocess(image)\n\n        # Tokenize question\n        tokens = self.text_tokenizer(question, padding='max_length', truncation=True)\n        tokens.input_ids = tokens.input_ids.squeeze()\n        tokens.attention_mask = tokens.attention_mask.squeeze()\n        image = self.image_processor(image)\n        return {\n            'image': image,\n            'questions': tokens,\n            'answer': answer\n        }\n\nbatch_size = 32\n    \n# Assuming you have separate dataframes for training and validation\ntrain_data = VQADataset(train_df, train_dataset)\nval_data = VQADataset(val_df, train_dataset)\n\n# DataLoader for training and validation\ntrain_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_data, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T09:20:47.117159Z","iopub.execute_input":"2024-05-17T09:20:47.117580Z","iopub.status.idle":"2024-05-17T09:20:48.485015Z","shell.execute_reply.started":"2024-05-17T09:20:47.117548Z","shell.execute_reply":"2024-05-17T09:20:48.484277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nwith open('/kaggle/input/vqayesno/small_ans_dictionaries_final.pkl', 'rb') as f:\n    data = pickle.load(f)\n    id_to_answer = data['id_to_answer']\n    answer_to_id = data['answer_to_id']\n\n# print(\"Dictionaries have been loaded from answers_dictionaries.pkl\")\n# print(\"ID to Answer Dictionary:\", id_to_answer)\n# print(\"Answer dede bhai: \",Â answer_to_id)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T09:20:54.293385Z","iopub.execute_input":"2024-05-17T09:20:54.294161Z","iopub.status.idle":"2024-05-17T09:20:54.306574Z","shell.execute_reply.started":"2024-05-17T09:20:54.294126Z","shell.execute_reply":"2024-05-17T09:20:54.305672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert len(answer_to_id) == len(id_to_answer)\nmodel = VQAModel(num_classes=len(answer_to_id))","metadata":{"execution":{"iopub.status.busy":"2024-05-17T09:20:55.830290Z","iopub.execute_input":"2024-05-17T09:20:55.830650Z","iopub.status.idle":"2024-05-17T09:21:01.228077Z","shell.execute_reply.started":"2024-05-17T09:20:55.830622Z","shell.execute_reply":"2024-05-17T09:21:01.227134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(answer_to_id))","metadata":{"execution":{"iopub.status.busy":"2024-05-17T09:21:04.607623Z","iopub.execute_input":"2024-05-17T09:21:04.607958Z","iopub.status.idle":"2024-05-17T09:21:04.612881Z","shell.execute_reply.started":"2024-05-17T09:21:04.607932Z","shell.execute_reply":"2024-05-17T09:21:04.611845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(answer_to_id)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T09:21:07.559942Z","iopub.execute_input":"2024-05-17T09:21:07.560820Z","iopub.status.idle":"2024-05-17T09:21:07.565227Z","shell.execute_reply.started":"2024-05-17T09:21:07.560785Z","shell.execute_reply":"2024-05-17T09:21:07.564144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[train_df['answers'].isna()]","metadata":{"execution":{"iopub.status.busy":"2024-05-17T09:21:09.608292Z","iopub.execute_input":"2024-05-17T09:21:09.608620Z","iopub.status.idle":"2024-05-17T09:21:09.629010Z","shell.execute_reply.started":"2024-05-17T09:21:09.608597Z","shell.execute_reply":"2024-05-17T09:21:09.628064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.tensorboard import SummaryWriter\n\n# Create a SummaryWriter object\nwriter = SummaryWriter('runs/experiment_1_yesno')\nnum_epochs = 10\nmodel.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-17T09:21:14.696619Z","iopub.execute_input":"2024-05-17T09:21:14.697526Z","iopub.status.idle":"2024-05-17T09:21:14.733977Z","shell.execute_reply.started":"2024-05-17T09:21:14.697493Z","shell.execute_reply":"2024-05-17T09:21:14.733120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# i = 0\n# for epoch in range(100):\n#     for batch in train_dataloader:\n#         # Get the inputs and targets from the batch\n#         images = batch['image']['pixel_values']\n#         questions = batch['questions']\n#         answers = batch['answer']\n# #         print(questions.input_ids.shape)\n# #         print(images)\n# #         questions.input_ids = questions.input_ids.squeeze(1)\n#         questions.input_ids = questions.input_ids.squeeze()\n#         questions.attention_mask = questions.attention_mask.squeeze()\n#         print(questions.input_ids.shape)\n#         TextEmbedding()(questions)\n#         i += 1\n#         if i > 0: break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_dataset[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df[train_df['index'] == 0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n# loss = criterion(out, tar)\n# loss.backward()\n# optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2024-05-17T09:21:26.517993Z","iopub.execute_input":"2024-05-17T09:21:26.518981Z","iopub.status.idle":"2024-05-17T09:21:26.526195Z","shell.execute_reply.started":"2024-05-17T09:21:26.518948Z","shell.execute_reply":"2024-05-17T09:21:26.525061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_ref = 100","metadata":{"execution":{"iopub.status.busy":"2024-05-17T09:21:29.980618Z","iopub.execute_input":"2024-05-17T09:21:29.981293Z","iopub.status.idle":"2024-05-17T09:21:29.985373Z","shell.execute_reply.started":"2024-05-17T09:21:29.981263Z","shell.execute_reply":"2024-05-17T09:21:29.984236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchmetrics.classification import Precision, Recall, Accuracy, F1Score, AUROC","metadata":{"execution":{"iopub.status.busy":"2024-05-17T09:21:32.224880Z","iopub.execute_input":"2024-05-17T09:21:32.225743Z","iopub.status.idle":"2024-05-17T09:21:34.122880Z","shell.execute_reply.started":"2024-05-17T09:21:32.225713Z","shell.execute_reply":"2024-05-17T09:21:34.122110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precision_metric = Precision(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\nrecall_metric = Recall(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\naccuracy_metric = Accuracy(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\nf1_metric = F1Score(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n# auroc_metric = AUROC(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n\ndef evaluate(preds, true):\n    p = precision_metric(preds, true)\n    r = recall_metric(preds, true)\n    a = accuracy_metric(preds, true)\n    f = f1_metric(preds, true)\n#     am = auroc_metric(preds, true)\n    \n    return {\n        \"precision\": p,\n        \"recall\": r,\n        \"accuracy\": a,\n        \"f1\": f,\n#         \"auroc\": auroc_metric\n    }\n","metadata":{"execution":{"iopub.status.busy":"2024-05-17T09:21:36.600970Z","iopub.execute_input":"2024-05-17T09:21:36.601723Z","iopub.status.idle":"2024-05-17T09:21:36.613639Z","shell.execute_reply.started":"2024-05-17T09:21:36.601691Z","shell.execute_reply":"2024-05-17T09:21:36.612576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\n\nbatch_no = 0\navg_accuracy = 0\n\nfor epoch in range(num_epochs):\n    batch_no = 0\n    avg_accuracy = 0\n    for batch in train_dataloader:\n        # Get the inputs and targets from the batch\n        images = batch['image']\n        questions = batch['questions']\n        answers = batch['answer']\n\n        questions.input_ids = questions.input_ids.squeeze()\n        questions.attention_mask = questions.attention_mask.squeeze()\n        images.pixel_values = images.pixel_values.squeeze()\n        \n        # Forward pass\n        outputs = model(questions, images)\n\n        \n        answers_ids = [answer_to_id[key]-1 for key in answers]\n        answers_ids_tensor = torch.tensor(answers_ids)\n#         print(answers_ids_tensor)\n        answers = F.one_hot(answers_ids_tensor, num_classes=len(answer_to_id)).to(outputs.dtype)\n        answers = answers.to(device)\n        \n        # Sanity check\n        assert answers.shape == outputs.shape, \"Target and Predicted shapes don't match\"\n    \n        # Compute the loss\n        loss = criterion(outputs, answers)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        \n        _, preds = torch.max(outputs, 1)\n        _, answer_inds = torch.max(answers, 1)\n        \n        assert preds.shape == answer_inds.shape, f\"Preds_shape: {preds.shape}, Answers_shape: {answer_inds.shape}\"\n        eval_met = evaluate(preds, answer_inds)\n        \n        iter_val = epoch * len(train_dataloader) + batch_no\n        \n        \n        \n        writer.add_scalar('Training Loss', loss.item(), iter_val)\n        writer.add_pr_curve('PR Curve', answers, outputs, iter_val)\n        writer.add_scalar('Accuracy', eval_met['accuracy'], iter_val)\n        writer.add_scalar('Precision', eval_met['precision'], iter_val)\n        writer.add_scalar('Recall', eval_met['recall'], iter_val)\n        writer.add_scalar('F1', eval_met['f1'], iter_val)\n        \n        \n        if batch_no % checkpoint_ref == 0:\n            torch.save(model.state_dict(), f\"./checkpoints/{epoch}.pth\")\n            \n        batch_no += 1\n        print(f\"Batch -> {batch_no} done -> Accu: {eval_met['accuracy']:.2f} -> Prec: {eval_met['precision']:.2f}\\r\", end=\"\")\n        avg_accuracy += eval_met['accuracy']\n        \n\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Avg. Epoch Acc: {avg_accuracy / 1876}')\n# writer.close()","metadata":{"execution":{"iopub.status.busy":"2024-05-17T09:21:42.483306Z","iopub.execute_input":"2024-05-17T09:21:42.483961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_data[0]['questions'].input_ids.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list(answer_to_id.keys())[8063]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # len(train_df[~train_df['answers'].isna()])\n# train_df = train_df[~train_df['answers'].isna()]\n# val_df = val_df[~val_df['answers'].isna()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# len(train_df), len(val_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# _uniq = pd.concat([train_df['answers'], val_df['answers']])\n# tot = 0\n\n# for key in _uniq:\n#     tot += 1\n#     try:\n#         _ids = [answer_to_id[key]]\n#     except:\n#         print(key, type(key), tot)\n# print(tot)        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# question = TextTokenizer()(\"Hi there\")\n# (question)\n# TextEmbedding()(question)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# question.input_ids.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# i = 0\n# for epoch in range(num_epochs):\n#     for batch in train_dataloader:\n#         # Get the inputs and targets from the batch\n#         images = batch['image']['pixel_values']\n#         questions = batch['questions']\n#         answers = batch['answer']\n# #         print(questions.input_ids.shape)\n# #         print(images)\n# #         questions.input_ids = questions.input_ids.squeeze(1)\n#         print(images.shape)\n#         TextEmbedding()(questions)\n#         i += 1\n#         if i > 0: break","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}