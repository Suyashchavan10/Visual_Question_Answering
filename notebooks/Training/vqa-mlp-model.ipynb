{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Training Coattention Model"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-18T08:22:17.483443Z","iopub.status.busy":"2024-05-18T08:22:17.483161Z","iopub.status.idle":"2024-05-18T08:22:24.215890Z","shell.execute_reply":"2024-05-18T08:22:24.214917Z","shell.execute_reply.started":"2024-05-18T08:22:17.483419Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-18 08:22:21.398514: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-18 08:22:21.398567: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-18 08:22:21.399973: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["from transformers import BertModel, BertTokenizer, ViTImageProcessor, ViTModel\n","import torch\n","from torchinfo import summary\n","from torch import nn\n","from torch.nn import Transformer, TransformerDecoder, TransformerDecoderLayer, TransformerEncoder, TransformerEncoderLayer"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T08:22:24.222164Z","iopub.status.busy":"2024-05-18T08:22:24.221884Z","iopub.status.idle":"2024-05-18T08:22:24.261771Z","shell.execute_reply":"2024-05-18T08:22:24.260800Z","shell.execute_reply.started":"2024-05-18T08:22:24.222138Z"},"trusted":true},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# device = torch.device('cpu')"]},{"cell_type":"markdown","metadata":{},"source":["## Initialize Necessary Modules"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T08:22:24.263409Z","iopub.status.busy":"2024-05-18T08:22:24.263121Z","iopub.status.idle":"2024-05-18T08:22:24.277868Z","shell.execute_reply":"2024-05-18T08:22:24.276959Z","shell.execute_reply.started":"2024-05-18T08:22:24.263384Z"},"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["device"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T08:22:24.279318Z","iopub.status.busy":"2024-05-18T08:22:24.279023Z","iopub.status.idle":"2024-05-18T08:22:24.292285Z","shell.execute_reply":"2024-05-18T08:22:24.291440Z","shell.execute_reply.started":"2024-05-18T08:22:24.279292Z"},"trusted":true},"outputs":[],"source":["class TextTokenizer(torch.nn.Module):\n","    def __init__(\n","        self,\n","        text_tokenizer=BertTokenizer,\n","        max_length=25  # Add a max_length parameter\n","    ):\n","        super().__init__()\n","        self.text_tokenizer = text_tokenizer.from_pretrained('bert-base-uncased')\n","        self.max_length = max_length  # Store the max_length\n","\n","    def forward(self, input_question, padding='max_length', truncation=True):\n","        tokens = self.text_tokenizer(input_question, return_tensors='pt', \n","                                     padding=padding, truncation=truncation, \n","                                     max_length=self.max_length).to(device)  # Use max_length\n","\n","        return tokens\n","\n","class ImageProcessor(torch.nn.Module):\n","    def __init__(\n","        self,\n","        image_model_processor=ViTImageProcessor\n","    ):\n","\n","        super().__init__()\n","        self.image_model_processor = image_model_processor.from_pretrained('google/vit-base-patch16-224-in21k')\n","\n","    def forward(self, image):\n","        image = self.image_model_processor(image, return_tensors='pt').to(device)\n","\n","        return image\n","\n","class TextEmbedding(torch.nn.Module):\n","    def __init__(\n","        self,\n","        text_model=BertModel,\n","    ):\n","        super().__init__()\n","        self.text_model = text_model.from_pretrained('bert-base-uncased').to(device)\n","\n","\n","    def forward(self, tokens):\n","        text_output = self.text_model(input_ids=tokens.input_ids, attention_mask=tokens.attention_mask)\n","        text_output = text_output.last_hidden_state     # CLS token from the last layer\n","\n","        return text_output\n","\n","\n","class ImageEmbedding(torch.nn.Module):\n","    def __init__(\n","            self, \n","            image_model=ViTModel\n","        ):\n","        \n","        super().__init__()\n","        self.image_model = image_model.from_pretrained('google/vit-base-patch16-224-in21k').to(device)\n","\n","\n","    def forward(self, image):\n","        image_output = self.image_model(pixel_values=image.pixel_values).last_hidden_state\n","\n","        return image_output"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T08:22:24.296047Z","iopub.status.busy":"2024-05-18T08:22:24.295770Z","iopub.status.idle":"2024-05-18T08:22:24.320827Z","shell.execute_reply":"2024-05-18T08:22:24.319940Z","shell.execute_reply.started":"2024-05-18T08:22:24.296015Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'NotebookApp': {'iopub_msg_rate_limit': 10000, 'rate_limit_window': 10.0}}"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["from notebook.services.config import ConfigManager\n","cm = ConfigManager()\n","cm.update('notebook', {\n","    'NotebookApp': {\n","        'iopub_msg_rate_limit': 10000,\n","        'rate_limit_window': 10.0\n","    }\n","})"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T08:22:24.322702Z","iopub.status.busy":"2024-05-18T08:22:24.322058Z","iopub.status.idle":"2024-05-18T08:22:26.264390Z","shell.execute_reply":"2024-05-18T08:22:26.263305Z","shell.execute_reply.started":"2024-05-18T08:22:24.322669Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for HuggingFaceM4/VQAv2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/HuggingFaceM4/VQAv2\n","You can avoid this message in future by passing the argument `trust_remote_code=True`.\n","Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n","  warnings.warn(\n","Repo card metadata block was not found. Setting CardData to empty.\n"]}],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"HuggingFaceM4/VQAv2\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T08:22:26.266934Z","iopub.status.busy":"2024-05-18T08:22:26.266149Z","iopub.status.idle":"2024-05-18T08:22:26.271140Z","shell.execute_reply":"2024-05-18T08:22:26.270201Z","shell.execute_reply.started":"2024-05-18T08:22:26.266895Z"},"trusted":true},"outputs":[],"source":["train_dataset = dataset['train']\n","# test_dataset = dataset['test']\n","# val_dataset = dataset['validation']"]},{"cell_type":"markdown","metadata":{},"source":["## Load Dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T08:22:26.272467Z","iopub.status.busy":"2024-05-18T08:22:26.272211Z","iopub.status.idle":"2024-05-18T08:22:26.606641Z","shell.execute_reply":"2024-05-18T08:22:26.605531Z","shell.execute_reply.started":"2024-05-18T08:22:26.272445Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","train_df = pd.read_csv('/kaggle/input/vqadataset/vqa_train_dataset.csv')\n","val_df = pd.read_csv('/kaggle/input/vqadataset/vqa_val_dataset.csv')\n","\n","train_df = train_df[~train_df['answers'].isna()]\n","val_df = val_df[~val_df['answers'].isna()]"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T08:22:26.608340Z","iopub.status.busy":"2024-05-18T08:22:26.608047Z","iopub.status.idle":"2024-05-18T08:22:27.243857Z","shell.execute_reply":"2024-05-18T08:22:27.242856Z","shell.execute_reply.started":"2024-05-18T08:22:26.608316Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor\n","\n","class VQADataset(Dataset):\n","    def __init__(self, dataframe, image_dataset):\n","        self.dataframe = dataframe\n","        self.image_dataset = image_dataset\n","        self.text_tokenizer = TextTokenizer()\n","        self.image_processor = ImageProcessor()\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        row = self.dataframe.iloc[idx]\n","        \n","        ind = int(row['index'])\n","        image = self.image_dataset[ind]['image']\n","        question = row['question']\n","        answer = row['answers']\n","        \n","        # sanity check        \n","        assert self.image_dataset[ind]['question'] == question, \"Mismatching training and Image data\"\n","\n","        \n","        image = image.convert('RGB')\n","\n","        tokens = self.text_tokenizer(question, padding='max_length', truncation=True)\n","        tokens.input_ids = tokens.input_ids.squeeze()\n","        tokens.attention_mask = tokens.attention_mask.squeeze()\n","        image = self.image_processor(image)\n","        return {\n","            'image': image,\n","            'questions': tokens,\n","            'answer': answer\n","        }\n","\n","batch_size = 64\n","    \n","train_data = VQADataset(train_df, train_dataset)\n","val_data = VQADataset(val_df, train_dataset)\n","\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T08:22:27.245354Z","iopub.status.busy":"2024-05-18T08:22:27.245061Z","iopub.status.idle":"2024-05-18T08:22:27.254120Z","shell.execute_reply":"2024-05-18T08:22:27.253293Z","shell.execute_reply.started":"2024-05-18T08:22:27.245323Z"},"trusted":true},"outputs":[],"source":["import pickle\n","\n","with open('/kaggle/input/vqadataset/answers_dictionaries.pkl', 'rb') as f:\n","    data = pickle.load(f)\n","    id_to_answer = data['id_to_answer']\n","    answer_to_id = data['answer_to_id']\n","\n","# print(\"Dictionaries have been loaded from answers_dictionaries.pkl\")\n","# print(\"ID to Answer Dictionary:\", id_to_answer)\n","# print(\"Answer dede bhai: \", answer_to_id)"]},{"cell_type":"markdown","metadata":{},"source":["## Model Creation"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T08:22:27.255838Z","iopub.status.busy":"2024-05-18T08:22:27.255493Z","iopub.status.idle":"2024-05-18T08:22:27.268419Z","shell.execute_reply":"2024-05-18T08:22:27.267510Z","shell.execute_reply.started":"2024-05-18T08:22:27.255808Z"},"trusted":true},"outputs":[],"source":["class VQAModel(nn.Module):\n","    def __init__(\n","        self,\n","        dim_model = 768,      # image and text embeddings concatenated\n","        nhead = 12,                    # No. of Attention heads\n","        num_layers = 1,               # No. of encoder layers\n","        num_classes = 8000\n","    ):\n","        super().__init__()\n","        self.text_embedder = TextEmbedding()\n","        self.image_embedder = ImageEmbedding()\n","\n","        self.mlp = nn.Sequential(\n","            nn.Linear(2*dim_model, dim_model),\n","            nn.ReLU(),\n","            nn.Dropout(0.4),\n","        ).to(device)\n","\n","        self.classifier = nn.Linear(dim_model, num_classes).to(device)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","\n","    def forward(self, questions, images):\n","        question_embedding = self.text_embedder(questions)\n","        image_embedding = self.image_embedder(images)\n","        question_embedding = question_embedding[:, 0, :]\n","        image_embedding = image_embedding[:, 0, :]\n","\n","        # Concatenate embeddings\n","        embeddings = torch.cat((question_embedding, image_embedding), dim=1)  # (batch_size, 2*dim_model)\n","\n","        output = self.mlp(embeddings)  # (batch_size, dim_model)\n"," \n","        logits = self.classifier(output)  # (batch_size, num_classes)\n","\n","        output = self.softmax(logits)  # (batch_size, num_classes)\n","        \n","\n","\n","        return output\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T08:22:27.291954Z","iopub.status.busy":"2024-05-18T08:22:27.291319Z","iopub.status.idle":"2024-05-18T08:22:28.577039Z","shell.execute_reply":"2024-05-18T08:22:28.576164Z","shell.execute_reply.started":"2024-05-18T08:22:27.291921Z"},"trusted":true},"outputs":[],"source":["model = VQAModel(num_classes=len(answer_to_id))"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T08:22:28.578596Z","iopub.status.busy":"2024-05-18T08:22:28.578227Z","iopub.status.idle":"2024-05-18T08:22:28.583943Z","shell.execute_reply":"2024-05-18T08:22:28.582170Z","shell.execute_reply.started":"2024-05-18T08:22:28.578563Z"},"trusted":true},"outputs":[],"source":["# print([(n, type(m)) for n, m in model().named_modules()])\n","# !pip install peft"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T08:22:28.586223Z","iopub.status.busy":"2024-05-18T08:22:28.585810Z","iopub.status.idle":"2024-05-18T08:22:28.597883Z","shell.execute_reply":"2024-05-18T08:22:28.597021Z","shell.execute_reply.started":"2024-05-18T08:22:28.586187Z"},"trusted":true},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.003)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T08:22:28.599195Z","iopub.status.busy":"2024-05-18T08:22:28.598920Z","iopub.status.idle":"2024-05-18T08:22:28.622421Z","shell.execute_reply":"2024-05-18T08:22:28.621496Z","shell.execute_reply.started":"2024-05-18T08:22:28.599171Z"},"trusted":true},"outputs":[{"data":{"text/plain":["VQAModel(\n","  (text_embedder): TextEmbedding(\n","    (text_model): BertModel(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0-11): 12 x BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","  )\n","  (image_embedder): ImageEmbedding(\n","    (image_model): ViTModel(\n","      (embeddings): ViTEmbeddings(\n","        (patch_embeddings): ViTPatchEmbeddings(\n","          (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (encoder): ViTEncoder(\n","        (layer): ModuleList(\n","          (0-11): 12 x ViTLayer(\n","            (attention): ViTAttention(\n","              (attention): ViTSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","              (output): ViTSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","            )\n","            (intermediate): ViTIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): ViTOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          )\n","        )\n","      )\n","      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (pooler): ViTPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","  )\n","  (mlp): Sequential(\n","    (0): Linear(in_features=1536, out_features=768, bias=True)\n","    (1): ReLU()\n","    (2): Dropout(p=0.4, inplace=False)\n","  )\n","  (classifier): Linear(in_features=768, out_features=8161, bias=True)\n","  (softmax): Softmax(dim=1)\n",")"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["from torch.utils.tensorboard import SummaryWriter\n","\n","# Create a SummaryWriter object\n","writer = SummaryWriter('runs/experiment_mlp')\n","num_epochs = 3\n","model.train()"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T08:22:28.626219Z","iopub.status.busy":"2024-05-18T08:22:28.625957Z","iopub.status.idle":"2024-05-18T08:22:28.630183Z","shell.execute_reply":"2024-05-18T08:22:28.629328Z","shell.execute_reply.started":"2024-05-18T08:22:28.626198Z"},"trusted":true},"outputs":[],"source":["checkpoint_ref = 100"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation Metrics"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T08:22:28.632022Z","iopub.status.busy":"2024-05-18T08:22:28.631740Z","iopub.status.idle":"2024-05-18T08:22:30.201409Z","shell.execute_reply":"2024-05-18T08:22:30.200605Z","shell.execute_reply.started":"2024-05-18T08:22:28.631999Z"},"trusted":true},"outputs":[],"source":["from torchmetrics.classification import Precision, Recall, Accuracy, F1Score, AUROC\n","\n","precision_metric = Precision(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n","recall_metric = Recall(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n","accuracy_metric = Accuracy(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n","f1_metric = F1Score(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n","# auroc_metric = AUROC(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n","\n","def evaluate(preds, true):\n","    p = precision_metric(preds, true)\n","    r = recall_metric(preds, true)\n","    a = accuracy_metric(preds, true)\n","    f = f1_metric(preds, true)\n","#     am = auroc_metric(preds, true)\n","    \n","    return {\n","        \"precision\": p,\n","        \"recall\": r,\n","        \"accuracy\": a,\n","        \"f1\": f,\n","#         \"auroc\": auroc_metric\n","    }"]},{"cell_type":"markdown","metadata":{},"source":["## Training Loop"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T08:22:30.203402Z","iopub.status.busy":"2024-05-18T08:22:30.202659Z","iopub.status.idle":"2024-05-18T08:22:30.207846Z","shell.execute_reply":"2024-05-18T08:22:30.206845Z","shell.execute_reply.started":"2024-05-18T08:22:30.203362Z"},"trusted":true},"outputs":[],"source":["# !mkdir ./checkpoints"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T08:22:30.209263Z","iopub.status.busy":"2024-05-18T08:22:30.208991Z","iopub.status.idle":"2024-05-18T11:42:48.842631Z","shell.execute_reply":"2024-05-18T11:42:48.841643Z","shell.execute_reply.started":"2024-05-18T08:22:30.209239Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/3, Loss: 8.815025329589844, Avg. Epoch Acc: 0.19511090219020844\n","Epoch 2/3, Loss: 8.757333755493164, Avg. Epoch Acc: 0.19554166495800018\n","Epoch 3/3, Loss: 8.699640274047852, Avg. Epoch Acc: 0.1955474317073822\n"]}],"source":["import torch.nn.functional as F\n","\n","batch_no = 0\n","avg_accuracy = 0\n","\n","for epoch in range(num_epochs):\n","    batch_no = 0\n","    avg_accuracy = 0\n","    for batch in train_dataloader:\n","        # Get the inputs and targets from the batch\n","        images = batch['image']\n","        questions = batch['questions']\n","        answers = batch['answer']\n","\n","        questions.input_ids = questions.input_ids.squeeze()\n","        questions.attention_mask = questions.attention_mask.squeeze()\n","        images.pixel_values = images.pixel_values.squeeze()\n","        \n","        # Forward pass\n","        outputs = model(questions, images)\n","\n","        \n","        answers_ids = [answer_to_id[key] for key in answers]\n","        answers_ids_tensor = torch.tensor(answers_ids)\n","        answers = F.one_hot(answers_ids_tensor, num_classes=len(answer_to_id)).to(outputs.dtype)\n","        answers = answers.to(device)\n","    \n","        \n","        # Sanity check\n","#         print(answers.shape, outputs.shape)\n","        assert answers.shape == outputs.shape, \"Target and Predicted shapes don't match\"\n","    \n","        \n","        # Compute the loss\n","        loss = criterion(outputs, answers)\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        \n","        _, preds = torch.max(outputs, 1)\n","        _, answer_inds = torch.max(answers, 1)\n","        \n","        assert preds.shape == answer_inds.shape, f\"Preds_shape: {preds.shape}, Answers_shape: {answer_inds.shape}\"\n","        eval_met = evaluate(preds, answer_inds)\n","        \n","        iter_val = epoch * len(train_dataloader) + batch_no\n","        \n","        \n","        writer.add_scalar('Training Loss', loss.item(), iter_val)\n","        writer.add_pr_curve('PR Curve', answers, outputs, iter_val)\n","        writer.add_scalar('Accuracy', eval_met['accuracy'], iter_val)\n","        writer.add_scalar('Precision', eval_met['precision'], iter_val)\n","        writer.add_scalar('Recall', eval_met['recall'], iter_val)\n","        writer.add_scalar('F1', eval_met['f1'], iter_val)\n","         \n","        \n","        if batch_no % checkpoint_ref == 0:\n","            torch.save(model.state_dict(), f\"./checkpoints/mlp.pth\")\n","            \n","        batch_no += 1\n","        print(f\"Batch -> {batch_no} done -> Accu: {eval_met['accuracy']:.2f} -> Prec: {eval_met['precision']:.2f}\\r\", end=\"\")\n","        avg_accuracy += eval_met['accuracy']\n","        \n","\n","    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Avg. Epoch Acc: {avg_accuracy / len(train_dataloader)}')\n","# writer.close()"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T11:42:48.844129Z","iopub.status.busy":"2024-05-18T11:42:48.843825Z","iopub.status.idle":"2024-05-18T11:42:48.848381Z","shell.execute_reply":"2024-05-18T11:42:48.847454Z","shell.execute_reply.started":"2024-05-18T11:42:48.844104Z"},"trusted":true},"outputs":[],"source":["# model.transformer_encoder_text_1.state_dict()['layers.0.self_attn.in_proj_weight']"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5024895,"sourceId":8436330,"sourceType":"datasetVersion"},{"sourceId":178189965,"sourceType":"kernelVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
