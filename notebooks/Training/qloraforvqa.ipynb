{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8432239,"sourceType":"datasetVersion","datasetId":5021895}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import BertModel, BertTokenizer, ViTImageProcessor, ViTModel\nimport torch\nfrom torchinfo import summary\nfrom torch import nn\nfrom torch.nn import Transformer, TransformerDecoder, TransformerDecoderLayer, TransformerEncoder, TransformerEncoderLayer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# device = torch.device('cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextTokenizer(torch.nn.Module):\n    def __init__(\n        self,\n        text_tokenizer=BertTokenizer,\n        max_length=25  # Add a max_length parameter\n    ):\n        super().__init__()\n        self.text_tokenizer = text_tokenizer.from_pretrained('bert-base-uncased')\n        self.max_length = max_length  # Store the max_length\n\n    def forward(self, input_question, padding='max_length', truncation=True):\n        tokens = self.text_tokenizer(input_question, return_tensors='pt', \n                                     padding=padding, truncation=truncation, \n                                     max_length=self.max_length).to(device)  # Use max_length\n\n        return tokens\n\nclass ImageProcessor(torch.nn.Module):\n    def __init__(\n        self,\n        image_model_processor=ViTImageProcessor\n    ):\n\n        super().__init__()\n        self.image_model_processor = image_model_processor.from_pretrained('google/vit-base-patch16-224-in21k')\n\n    def forward(self, image):\n        image = self.image_model_processor(image, return_tensors='pt').to(device)\n\n        return image\n\nclass TextEmbedding(torch.nn.Module):\n    def __init__(\n        self,\n        text_model=BertModel,\n    ):\n        super().__init__()\n        self.text_model = text_model.from_pretrained('bert-base-uncased').to(device)\n\n\n    def forward(self, tokens):\n        text_output = self.text_model(input_ids=tokens.input_ids, attention_mask=tokens.attention_mask)\n        text_output = text_output.last_hidden_state     # CLS token from the last layer\n\n        return text_output\n\n\nclass ImageEmbedding(torch.nn.Module):\n    def __init__(\n            self, \n            image_model=ViTModel\n        ):\n        \n        super().__init__()\n        self.image_model = image_model.from_pretrained('google/vit-base-patch16-224-in21k').to(device)\n\n\n    def forward(self, image):\n        image_output = self.image_model(pixel_values=image.pixel_values).last_hidden_state\n\n        return image_output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from notebook.services.config import ConfigManager\ncm = ConfigManager()\ncm.update('notebook', {\n    'NotebookApp': {\n        'iopub_msg_rate_limit': 10000,\n        'rate_limit_window': 10.0\n    }\n})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"HuggingFaceM4/VQAv2\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = dataset['train']\ntest_dataset = dataset['test']\nval_dataset = dataset['validation']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ntrain_df = pd.read_csv('/kaggle/input/vqadataset/vqa_train_dataset_final.csv')\nval_df = pd.read_csv('/kaggle/input/vqadataset/vqa_val_dataset_final.csv')\n\ntrain_df = train_df[~train_df['answers'].isna()]\nval_df = val_df[~val_df['answers'].isna()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom torchvision.transforms import Compose, Resize, CenterCrop, ToTensor\n\nclass VQADataset(Dataset):\n    def __init__(self, dataframe, image_dataset):\n        self.dataframe = dataframe\n        self.image_dataset = image_dataset\n        self.text_tokenizer = TextTokenizer()\n        self.image_processor = ImageProcessor()\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        \n        ind = int(row['index'])\n        image = self.image_dataset[ind]['image']\n        question = row['question']\n        answer = row['answers']\n        \n        # sanity check        \n        assert self.image_dataset[ind]['question'] == question, \"Mismatching training and Image data\"\n\n        \n        image = image.convert('RGB')\n        \n#         Preprocessing done in ImageEmbedder\n#         preprocess = Compose([\n#             Resize((224, 224)),\n#             CenterCrop(224),\n#             ToTensor(),\n#         ])\n#         image = preprocess(image)\n\n        # Tokenize question\n        tokens = self.text_tokenizer(question, padding='max_length', truncation=True)\n        tokens.input_ids = tokens.input_ids.squeeze()\n        tokens.attention_mask = tokens.attention_mask.squeeze()\n        image = self.image_processor(image)\n        return {\n            'image': image,\n            'questions': tokens,\n            'answer': answer\n        }\n\nbatch_size = 64\n    \n# Assuming you have separate dataframes for training and validation\ntrain_data = VQADataset(train_df, train_dataset)\nval_data = VQADataset(val_df, train_dataset)\n\n# DataLoader for training and validation\ntrain_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_data, batch_size=batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nwith open('/kaggle/input/vqadataset/answers_dictionaries_final.pkl', 'rb') as f:\n    data = pickle.load(f)\n    id_to_answer = data['id_to_answer']\n    answer_to_id = data['answer_to_id']\n\n# print(\"Dictionaries have been loaded from answers_dictionaries.pkl\")\n# print(\"ID to Answer Dictionary:\", id_to_answer)\n# print(\"Answer dede bhai: \",Â answer_to_id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VQAModel(nn.Module):\n    def __init__(\n        self,\n        dim_model = 768,      # image and text embeddings concatenated\n        nhead = 12,                    # No. of Attention heads\n        num_layers = 1,               # No. of encoder layers\n        num_classes = 8161\n    ):\n        super().__init__()\n        self.text_embedder = TextEmbedding()\n        self.image_embedder = ImageEmbedding()\n\n        encoder_layers = TransformerEncoderLayer(d_model=dim_model, nhead=nhead)\n        self.transformerEncoder = TransformerEncoder(encoder_layer=encoder_layers, num_layers=num_layers).to(device)\n\n        self.classifier = nn.Linear(dim_model, num_classes).to(device)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, questions, images):\n        question_embedding = self.text_embedder(questions)\n        image_embedding = self.image_embedder(images)\n\n        embeddings = torch.cat((question_embedding, image_embedding), dim=1)\n        embeddings = embeddings.permute(1, 0, 2)  # (seq, batch, feature)\n        output = self.transformerEncoder(embeddings)\n\n        cls_output = output[0, :, :]\n\n        logits = self.classifier(cls_output)\n        output = self.softmax(logits)\n\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = VQAModel(num_classes=len(answer_to_id))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print([(n, type(m)) for n, m in model.named_modules()])","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install peft","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_all_linear_layers(model):\n    linear_layers = []\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Linear):\n            linear_layers.append(name)\n    return linear_layers\n\nlinear_layers = find_all_linear_layers(model)\nprint(linear_layers)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Append the specific layers\nspecific_layers = [\n    \"image_embedder.image_model.encoder.layer.11.intermediate.dense\",\n    \"image_embedder.image_model.encoder.layer.11.output.dense\"\n]\nlinear_layers.extend(specific_layers)\nprint(linear_layers)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-05-18T12:38:30.230280Z","iopub.execute_input":"2024-05-18T12:38:30.230680Z","iopub.status.idle":"2024-05-18T12:38:42.667885Z","shell.execute_reply.started":"2024-05-18T12:38:30.230647Z","shell.execute_reply":"2024-05-18T12:38:42.666399Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.11.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2+cpu)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.39.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.29.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.22.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"# pip install bitsandbytes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pip install accelerate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pip install -i https://pypi.org/simple/ bitsandbytes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import BitsAndBytesConfig, AutoModel, ViTModel\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\n# Define the BitsAndBytesConfig for 4-bit quantization\nconfig = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\n# Load the pre-trained BERT model with quantization config\nbert_model = AutoModel.from_pretrained(\"bert-base-uncased\", quantization_config=config)\n\n# Load the pre-trained ViT model with quantization config\nvit_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\", quantization_config=config)\n\n# Prepare the models for k-bit training\nbert_model = prepare_model_for_kbit_training(bert_model)\nvit_model = prepare_model_for_kbit_training(vit_model)\n\n# Define the LORA configuration\nLORA_R = 8\nLORA_ALPHA = 512\nLORA_DROPOUT = 0.05\n\nlora_config = LoraConfig(\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    lora_dropout=LORA_DROPOUT,\n    bias=\"none\",\n    target_modules=[\n        \"encoder.layer.11.intermediate.dense\",\n        \"encoder.layer.11.output.dense\"\n    ]\n)\n\nbert_lora_model = get_peft_model(bert_model, bert_lora_config)\nvit_lora_model = get_peft_model(vit_model, vit_lora_config)\n\n# Print the trainable parameters for both models\nprint(\"BERT model trainable parameters:\")\nbert_lora_model.print_trainable_parameters()\n\nprint(\"ViT model trainable parameters:\")\nvit_lora_model.print_trainable_parameters()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-18T12:38:48.684368Z","iopub.execute_input":"2024-05-18T12:38:48.684828Z","iopub.status.idle":"2024-05-18T12:38:57.191414Z","shell.execute_reply.started":"2024-05-18T12:38:48.684790Z","shell.execute_reply":"2024-05-18T12:38:57.189669Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m      6\u001b[0m config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      7\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load the pre-trained BERT model with quantization config\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m bert_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Load the pre-trained ViT model with quantization config\u001b[39;00m\n\u001b[1;32m     17\u001b[0m vit_model \u001b[38;5;241m=\u001b[39m ViTModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/vit-base-patch16-224-in21k\u001b[39m\u001b[38;5;124m\"\u001b[39m, quantization_config\u001b[38;5;241m=\u001b[39mconfig)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3049\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3046\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3049\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3051\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3052\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3053\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:62\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_environment\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_accelerate_available() \u001b[38;5;129;01mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[0;32m---> 62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     63\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m         )\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_tf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_flax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m sure the weights are in PyTorch format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         )\n","\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`"],"ename":"ImportError","evalue":"Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`","output_type":"error"}]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\n# Define the LoRA configuration\nLORA_R = 8\nLORA_ALPHA = 512\nLORA_DROPOUT = 0.05\n\nlora_config = LoraConfig(\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    lora_dropout=LORA_DROPOUT,\n    bias=\"none\",\n    target_modules=[\n        \"transformerEncoder.layers.0.linear1\",\n        \"transformerEncoder.layers.0.linear2\",\n        \"image_embedder.image_model.encoder.layer.11.intermediate.dense\",\n        \"image_embedder.image_model.encoder.layer.11.output.dense\"\n    ]\n#     target_modules = linear_layers\n)\n\n# Initialize the VQALORAModel\nmodel = VQALORAModel()\n\n# Apply LoRA to the model\nlora_model = get_peft_model(model, lora_config)\n\n# Print the trainable parameters\nlora_model.print_trainable_parameters()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.003)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.tensorboard import SummaryWriter\n\n# Create a SummaryWriter object\nwriter = SummaryWriter('runs/exp_lora_layers_all')\nnum_epochs = 1\nlora_model.train()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_ref = 100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchmetrics.classification import Precision, Recall, Accuracy, F1Score, AUROC\n\nprecision_metric = Precision(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\nrecall_metric = Recall(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\naccuracy_metric = Accuracy(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\nf1_metric = F1Score(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n# auroc_metric = AUROC(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n\ndef evaluate(preds, true):\n    p = precision_metric(preds, true)\n    r = recall_metric(preds, true)\n    a = accuracy_metric(preds, true)\n    f = f1_metric(preds, true)\n#     am = auroc_metric(preds, true)\n    \n    return {\n        \"precision\": p,\n        \"recall\": r,\n        \"accuracy\": a,\n        \"f1\": f,\n#         \"auroc\": auroc_metric\n    }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\n\nbatch_no = 0\navg_accuracy = 0\n\nfor epoch in range(num_epochs):\n    batch_no = 0\n    avg_accuracy = 0\n    for batch in train_dataloader:\n        # Get the inputs and targets from the batch\n        images = batch['image']\n        questions = batch['questions']\n        answers = batch['answer']\n\n        questions.input_ids = questions.input_ids.squeeze()\n        questions.attention_mask = questions.attention_mask.squeeze()\n        images.pixel_values = images.pixel_values.squeeze()\n        \n        # Forward pass\n        outputs = lora_model(questions, images)\n\n        \n        answers_ids = [min(answer_to_id[key], len(answer_to_id)-1) for key in answers]\n        answers_ids_tensor = torch.tensor(answers_ids)\n        answers = F.one_hot(answers_ids_tensor, num_classes=len(answer_to_id)).to(outputs.dtype)\n        answers = answers.to(device)\n    \n        \n        # Sanity check\n        assert answers.shape == outputs.shape, \"Target and Predicted shapes don't match\"\n    \n        # Compute the loss\n        loss = criterion(outputs, answers)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        \n        _, preds = torch.max(outputs, 1)\n        _, answer_inds = torch.max(answers, 1)\n        \n        assert preds.shape == answer_inds.shape, f\"Preds_shape: {preds.shape}, Answers_shape: {answer_inds.shape}\"\n        eval_met = evaluate(preds, answer_inds)\n        \n        iter_val = epoch * len(train_dataloader) + batch_no\n        \n        \n        writer.add_scalar('Training Loss', loss.item(), iter_val)\n        writer.add_pr_curve('PR Curve', answers, outputs, iter_val)\n        writer.add_scalar('Accuracy', eval_met['accuracy'], iter_val)\n        writer.add_scalar('Precision', eval_met['precision'], iter_val)\n        writer.add_scalar('Recall', eval_met['recall'], iter_val)\n        writer.add_scalar('F1', eval_met['f1'], iter_val)\n        \n        \n        if batch_no % checkpoint_ref == 0:\n            torch.save(lora_model.state_dict(), f\"./loraLayersChecks/{epoch}.pth\")\n            \n        batch_no += 1\n        print(f\"Batch -> {batch_no} done -> Accu: {eval_met['accuracy']:.2f} -> Prec: {eval_met['precision']:.2f}\\r\", end=\"\")\n        avg_accuracy += eval_met['accuracy']\n        \n\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Avg. Epoch Acc: {avg_accuracy / 938}')\n# writer.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Start TensorBoard\n# %load_ext tensorboard\n# %tensorboard --logdir=runs/experiment_1_final","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}