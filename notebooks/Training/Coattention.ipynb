{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Training Coattention Model"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-18T05:28:11.599320Z","iopub.status.busy":"2024-05-18T05:28:11.599009Z","iopub.status.idle":"2024-05-18T05:28:18.258822Z","shell.execute_reply":"2024-05-18T05:28:18.257822Z","shell.execute_reply.started":"2024-05-18T05:28:11.599293Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-18 05:28:15.536763: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-18 05:28:15.536822: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-18 05:28:15.538221: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["from transformers import BertModel, BertTokenizer, ViTImageProcessor, ViTModel\n","import torch\n","from torchinfo import summary\n","from torch import nn\n","from torch.nn import Transformer, TransformerDecoder, TransformerDecoderLayer, TransformerEncoder, TransformerEncoderLayer"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T05:28:18.264104Z","iopub.status.busy":"2024-05-18T05:28:18.263825Z","iopub.status.idle":"2024-05-18T05:28:18.297874Z","shell.execute_reply":"2024-05-18T05:28:18.296972Z","shell.execute_reply.started":"2024-05-18T05:28:18.264079Z"},"trusted":true},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# device = torch.device('cpu')"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T05:28:18.299552Z","iopub.status.busy":"2024-05-18T05:28:18.299203Z","iopub.status.idle":"2024-05-18T05:28:18.310174Z","shell.execute_reply":"2024-05-18T05:28:18.309281Z","shell.execute_reply.started":"2024-05-18T05:28:18.299524Z"},"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["device"]},{"cell_type":"markdown","metadata":{},"source":["## Initialize Necessary Modules"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T05:28:18.313167Z","iopub.status.busy":"2024-05-18T05:28:18.312822Z","iopub.status.idle":"2024-05-18T05:28:18.325404Z","shell.execute_reply":"2024-05-18T05:28:18.324344Z","shell.execute_reply.started":"2024-05-18T05:28:18.313141Z"},"trusted":true},"outputs":[],"source":["class TextTokenizer(torch.nn.Module):\n","    def __init__(\n","        self,\n","        text_tokenizer=BertTokenizer,\n","        max_length=25  # Add a max_length parameter\n","    ):\n","        super().__init__()\n","        self.text_tokenizer = text_tokenizer.from_pretrained('bert-base-uncased')\n","        self.max_length = max_length  # Store the max_length\n","\n","    def forward(self, input_question, padding='max_length', truncation=True):\n","        tokens = self.text_tokenizer(input_question, return_tensors='pt', \n","                                     padding=padding, truncation=truncation, \n","                                     max_length=self.max_length).to(device)  # Use max_length\n","\n","        return tokens\n","\n","class ImageProcessor(torch.nn.Module):\n","    def __init__(\n","        self,\n","        image_model_processor=ViTImageProcessor\n","    ):\n","\n","        super().__init__()\n","        self.image_model_processor = image_model_processor.from_pretrained('google/vit-base-patch16-224-in21k')\n","\n","    def forward(self, image):\n","        image = self.image_model_processor(image, return_tensors='pt').to(device)\n","\n","        return image\n","\n","class TextEmbedding(torch.nn.Module):\n","    def __init__(\n","        self,\n","        text_model=BertModel,\n","    ):\n","        super().__init__()\n","        self.text_model = text_model.from_pretrained('bert-base-uncased').to(device)\n","\n","\n","    def forward(self, tokens):\n","        text_output = self.text_model(input_ids=tokens.input_ids, attention_mask=tokens.attention_mask)\n","        text_output = text_output.last_hidden_state     # CLS token from the last layer\n","\n","        return text_output\n","\n","\n","class ImageEmbedding(torch.nn.Module):\n","    def __init__(\n","            self, \n","            image_model=ViTModel\n","        ):\n","        \n","        super().__init__()\n","        self.image_model = image_model.from_pretrained('google/vit-base-patch16-224-in21k').to(device)\n","\n","\n","    def forward(self, image):\n","        image_output = self.image_model(pixel_values=image.pixel_values).last_hidden_state\n","\n","        return image_output"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T05:28:18.326832Z","iopub.status.busy":"2024-05-18T05:28:18.326555Z","iopub.status.idle":"2024-05-18T05:28:18.352695Z","shell.execute_reply":"2024-05-18T05:28:18.351752Z","shell.execute_reply.started":"2024-05-18T05:28:18.326806Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'NotebookApp': {'iopub_msg_rate_limit': 10000, 'rate_limit_window': 10.0}}"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["from notebook.services.config import ConfigManager\n","cm = ConfigManager()\n","cm.update('notebook', {\n","    'NotebookApp': {\n","        'iopub_msg_rate_limit': 10000,\n","        'rate_limit_window': 10.0\n","    }\n","})"]},{"cell_type":"markdown","metadata":{},"source":["## Load Dataset"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T05:28:18.354700Z","iopub.status.busy":"2024-05-18T05:28:18.354033Z","iopub.status.idle":"2024-05-18T05:28:20.771842Z","shell.execute_reply":"2024-05-18T05:28:20.770782Z","shell.execute_reply.started":"2024-05-18T05:28:18.354663Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for HuggingFaceM4/VQAv2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/HuggingFaceM4/VQAv2\n","You can avoid this message in future by passing the argument `trust_remote_code=True`.\n","Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n","  warnings.warn(\n","Repo card metadata block was not found. Setting CardData to empty.\n"]}],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"HuggingFaceM4/VQAv2\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T05:28:20.773988Z","iopub.status.busy":"2024-05-18T05:28:20.773396Z","iopub.status.idle":"2024-05-18T05:28:20.778285Z","shell.execute_reply":"2024-05-18T05:28:20.777124Z","shell.execute_reply.started":"2024-05-18T05:28:20.773960Z"},"trusted":true},"outputs":[],"source":["train_dataset = dataset['train']\n","# test_dataset = dataset['test']\n","# val_dataset = dataset['validation']"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T05:28:20.780086Z","iopub.status.busy":"2024-05-18T05:28:20.779770Z","iopub.status.idle":"2024-05-18T05:28:21.105351Z","shell.execute_reply":"2024-05-18T05:28:21.104544Z","shell.execute_reply.started":"2024-05-18T05:28:20.780060Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","train_df = pd.read_csv('/kaggle/input/vqadataset/vqa_train_dataset.csv')\n","val_df = pd.read_csv('/kaggle/input/vqadataset/vqa_val_dataset.csv')\n","\n","train_df = train_df[~train_df['answers'].isna()]\n","val_df = val_df[~val_df['answers'].isna()]"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T05:28:21.106765Z","iopub.status.busy":"2024-05-18T05:28:21.106483Z","iopub.status.idle":"2024-05-18T05:28:21.900861Z","shell.execute_reply":"2024-05-18T05:28:21.899824Z","shell.execute_reply.started":"2024-05-18T05:28:21.106741Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor\n","\n","class VQADataset(Dataset):\n","    def __init__(self, dataframe, image_dataset):\n","        self.dataframe = dataframe\n","        self.image_dataset = image_dataset\n","        self.text_tokenizer = TextTokenizer()\n","        self.image_processor = ImageProcessor()\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        row = self.dataframe.iloc[idx]\n","        \n","        ind = int(row['index'])\n","        image = self.image_dataset[ind]['image']\n","        question = row['question']\n","        answer = row['answers']\n","        \n","        # sanity check        \n","        assert self.image_dataset[ind]['question'] == question, \"Mismatching training and Image data\"\n","\n","        \n","        image = image.convert('RGB')\n","        \n","\n","        tokens = self.text_tokenizer(question, padding='max_length', truncation=True)\n","        tokens.input_ids = tokens.input_ids.squeeze()\n","        tokens.attention_mask = tokens.attention_mask.squeeze()\n","        image = self.image_processor(image)\n","        return {\n","            'image': image,\n","            'questions': tokens,\n","            'answer': answer\n","        }\n","\n","batch_size = 64\n","    \n","train_data = VQADataset(train_df, train_dataset)\n","val_data = VQADataset(val_df, train_dataset)\n","\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T05:28:21.902389Z","iopub.status.busy":"2024-05-18T05:28:21.902097Z","iopub.status.idle":"2024-05-18T05:28:21.911232Z","shell.execute_reply":"2024-05-18T05:28:21.910412Z","shell.execute_reply.started":"2024-05-18T05:28:21.902363Z"},"trusted":true},"outputs":[],"source":["import pickle\n","\n","with open('/kaggle/input/vqadataset/answers_dictionaries.pkl', 'rb') as f:\n","    data = pickle.load(f)\n","    id_to_answer = data['id_to_answer']\n","    answer_to_id = data['answer_to_id']\n","\n","# print(\"Dictionaries have been loaded from answers_dictionaries.pkl\")\n","# print(\"ID to Answer Dictionary:\", id_to_answer)\n","# print(\"Answer dede please: \", answer_to_id)"]},{"cell_type":"markdown","metadata":{},"source":["## Model Creation"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T05:28:21.923862Z","iopub.status.busy":"2024-05-18T05:28:21.923572Z","iopub.status.idle":"2024-05-18T05:28:21.942727Z","shell.execute_reply":"2024-05-18T05:28:21.941753Z","shell.execute_reply.started":"2024-05-18T05:28:21.923830Z"},"trusted":true},"outputs":[],"source":["class ViLBERTModel(torch.nn.Module):\n","    def __init__(\n","        self,\n","        dim_model=768,      # image and text embeddings concatenated\n","        nhead=12,                    # No. of Attention heads\n","        num_layers=1,               # No. of encoder layers\n","        num_classes=8000\n","    ):\n","        super().__init__()\n","        self.dim_model = dim_model\n","        self.text_embedder = TextEmbedding()\n","        self.image_embedder = ImageEmbedding()\n","        \n","        self.query = nn.Linear(self.dim_model, self.dim_model).to(device)\n","        self.key = nn.Linear(self.dim_model, self.dim_model).to(device)\n","        self.value = nn.Linear(self.dim_model, self.dim_model).to(device)\n","        \n","        self.co_attention_im = nn.MultiheadAttention(dim_model, nhead).to(device)\n","        self.co_attention_text = nn.MultiheadAttention(dim_model, nhead).to(device)\n","\n","        self.transformer_encoder_text_1 = nn.TransformerEncoder(\n","            nn.TransformerEncoderLayer(d_model=dim_model, nhead=nhead),\n","            num_layers=num_layers\n","        ).to(device)\n","\n","        self.transformer_encoder_image_1 = nn.TransformerEncoder(\n","            nn.TransformerEncoderLayer(d_model=dim_model, nhead=nhead),\n","            num_layers=num_layers\n","        ).to(device)\n","\n","        self.classifier = nn.Linear(dim_model, num_classes).to(device)\n","        self.softmax = nn.Softmax(dim=1).to(device)\n","\n","    def forward(self, questions, images):\n","        # Get text embeddings from BERT and ViT\n","        question_embedding = self.text_embedder(questions).transpose(0, 1).to(device)\n","        image_embedding = self.image_embedder(images).transpose(0, 1).to(device)\n","\n","        im_k, text_k = self.key(image_embedding), self.key(question_embedding)\n","        im_q, text_q = self.query(image_embedding), self.query(question_embedding)\n","        im_v, text_v = self.value(image_embedding), self.value(question_embedding)\n","        \n","        # skip connections\n","        im_k = im_k + image_embedding\n","        im_q = im_q + image_embedding\n","        im_v = im_v + image_embedding\n","        text_k = text_k + question_embedding\n","        text_q = text_q + question_embedding\n","        text_v = text_v + question_embedding\n","\n","        # First layer\n","        co_im, _ = self.co_attention_im(im_q, text_k, text_v)\n","        co_text, _ = self.co_attention_text(text_q, im_k, im_v)\n","        \n","        co_im = self.transformer_encoder_image_1(co_im)\n","        co_text = self.transformer_encoder_text_1(co_text)\n","        \n","        #skip connection\n","        co_im = co_im + image_embedding\n","        co_text = co_text + question_embedding\n","        \n","        embeddings = torch.cat((co_im, co_text), dim=0).transpose(0, 1)\n","        cls_output = embeddings[:, 0, :]\n","        output = self.classifier(cls_output)\n","        output = self.softmax(output)\n","        return output"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T05:28:21.947066Z","iopub.status.busy":"2024-05-18T05:28:21.946792Z","iopub.status.idle":"2024-05-18T05:28:23.372353Z","shell.execute_reply":"2024-05-18T05:28:23.371359Z","shell.execute_reply.started":"2024-05-18T05:28:21.947042Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"]}],"source":["model = ViLBERTModel(num_classes=len(answer_to_id))"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T05:28:23.374261Z","iopub.status.busy":"2024-05-18T05:28:23.373941Z","iopub.status.idle":"2024-05-18T05:28:23.378660Z","shell.execute_reply":"2024-05-18T05:28:23.377484Z","shell.execute_reply.started":"2024-05-18T05:28:23.374235Z"},"trusted":true},"outputs":[],"source":["# print([(n, type(m)) for n, m in model().named_modules()])\n","# !pip install peft"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T05:28:23.380429Z","iopub.status.busy":"2024-05-18T05:28:23.380056Z","iopub.status.idle":"2024-05-18T05:28:23.391538Z","shell.execute_reply":"2024-05-18T05:28:23.390240Z","shell.execute_reply.started":"2024-05-18T05:28:23.380401Z"},"trusted":true},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.003)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T05:28:23.417840Z","iopub.status.busy":"2024-05-18T05:28:23.417582Z","iopub.status.idle":"2024-05-18T05:28:23.421719Z","shell.execute_reply":"2024-05-18T05:28:23.420879Z","shell.execute_reply.started":"2024-05-18T05:28:23.417817Z"},"trusted":true},"outputs":[],"source":["checkpoint_ref = 100"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation Metrics"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T05:28:23.423231Z","iopub.status.busy":"2024-05-18T05:28:23.422941Z","iopub.status.idle":"2024-05-18T05:28:24.998316Z","shell.execute_reply":"2024-05-18T05:28:24.997307Z","shell.execute_reply.started":"2024-05-18T05:28:23.423207Z"},"trusted":true},"outputs":[],"source":["from torchmetrics.classification import Precision, Recall, Accuracy, F1Score, AUROC\n","\n","precision_metric = Precision(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n","recall_metric = Recall(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n","accuracy_metric = Accuracy(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n","f1_metric = F1Score(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n","# auroc_metric = AUROC(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n","\n","def evaluate(preds, true):\n","    p = precision_metric(preds, true)\n","    r = recall_metric(preds, true)\n","    a = accuracy_metric(preds, true)\n","    f = f1_metric(preds, true)\n","#     am = auroc_metric(preds, true)\n","    \n","    return {\n","        \"precision\": p,\n","        \"recall\": r,\n","        \"accuracy\": a,\n","        \"f1\": f,\n","#         \"auroc\": auroc_metric\n","    }"]},{"cell_type":"markdown","metadata":{},"source":["## Training Loop"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T05:28:24.999891Z","iopub.status.busy":"2024-05-18T05:28:24.999596Z","iopub.status.idle":"2024-05-18T05:28:25.977737Z","shell.execute_reply":"2024-05-18T05:28:25.976724Z","shell.execute_reply.started":"2024-05-18T05:28:24.999866Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: cannot create directory './checkpoints': File exists\n"]}],"source":["!mkdir ./checkpoints"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T05:28:23.393506Z","iopub.status.busy":"2024-05-18T05:28:23.393084Z","iopub.status.idle":"2024-05-18T05:28:23.416536Z","shell.execute_reply":"2024-05-18T05:28:23.415699Z","shell.execute_reply.started":"2024-05-18T05:28:23.393448Z"},"trusted":true},"outputs":[{"data":{"text/plain":["ViLBERTModel(\n","  (text_embedder): TextEmbedding(\n","    (text_model): BertModel(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0-11): 12 x BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","  )\n","  (image_embedder): ImageEmbedding(\n","    (image_model): ViTModel(\n","      (embeddings): ViTEmbeddings(\n","        (patch_embeddings): ViTPatchEmbeddings(\n","          (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (encoder): ViTEncoder(\n","        (layer): ModuleList(\n","          (0-11): 12 x ViTLayer(\n","            (attention): ViTAttention(\n","              (attention): ViTSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","              (output): ViTSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","            )\n","            (intermediate): ViTIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): ViTOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          )\n","        )\n","      )\n","      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (pooler): ViTPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","  )\n","  (query): Linear(in_features=768, out_features=768, bias=True)\n","  (key): Linear(in_features=768, out_features=768, bias=True)\n","  (value): Linear(in_features=768, out_features=768, bias=True)\n","  (co_attention_im): MultiheadAttention(\n","    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","  )\n","  (co_attention_text): MultiheadAttention(\n","    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","  )\n","  (transformer_encoder_text_1): TransformerEncoder(\n","    (layers): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","        )\n","        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n","        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.1, inplace=False)\n","        (dropout2): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","  )\n","  (transformer_encoder_image_1): TransformerEncoder(\n","    (layers): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","        )\n","        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n","        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.1, inplace=False)\n","        (dropout2): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","  )\n","  (classifier): Linear(in_features=768, out_features=8161, bias=True)\n","  (softmax): Softmax(dim=1)\n",")"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["from torch.utils.tensorboard import SummaryWriter\n","\n","# Create a SummaryWriter object\n","writer = SummaryWriter('runs/experiment_coatt')\n","num_epochs = 3\n","model.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T05:28:25.980198Z","iopub.status.busy":"2024-05-18T05:28:25.979793Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/3, Loss: 8.757332801818848, Avg. Epoch Acc: 0.19101667404174805\n","Epoch 2/3, Loss: 8.834256172180176, Avg. Epoch Acc: 0.19114230573177338\n","Batch -> 234 done -> Accu: 0.31 -> Prec: 0.31\r"]}],"source":["import torch.nn.functional as F\n","\n","batch_no = 0\n","avg_accuracy = 0\n","\n","for epoch in range(num_epochs):\n","    batch_no = 0\n","    avg_accuracy = 0\n","    for batch in train_dataloader:\n","        # Get the inputs and targets from the batch\n","        images = batch['image']\n","        questions = batch['questions']\n","        answers = batch['answer']\n","\n","        questions.input_ids = questions.input_ids.squeeze()\n","        questions.attention_mask = questions.attention_mask.squeeze()\n","        images.pixel_values = images.pixel_values.squeeze()\n","        \n","        # Forward pass\n","        outputs = model(questions, images)\n","\n","        \n","        answers_ids = [answer_to_id[key] for key in answers]\n","        answers_ids_tensor = torch.tensor(answers_ids)\n","        answers = F.one_hot(answers_ids_tensor, num_classes=len(answer_to_id)).to(outputs.dtype)\n","        answers = answers.to(device)\n","    \n","        \n","        # Sanity check\n","        assert answers.shape == outputs.shape, \"Target and Predicted shapes don't match\"\n","    \n","        # Compute the loss\n","        loss = criterion(outputs, answers)\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        \n","        _, preds = torch.max(outputs, 1)\n","        _, answer_inds = torch.max(answers, 1)\n","        \n","        assert preds.shape == answer_inds.shape, f\"Preds_shape: {preds.shape}, Answers_shape: {answer_inds.shape}\"\n","        eval_met = evaluate(preds, answer_inds)\n","        \n","        iter_val = epoch * len(train_dataloader) + batch_no\n","        \n","        \n","        writer.add_scalar('Training Loss', loss.item(), iter_val)\n","        writer.add_pr_curve('PR Curve', answers, outputs, iter_val)\n","        writer.add_scalar('Accuracy', eval_met['accuracy'], iter_val)\n","        writer.add_scalar('Precision', eval_met['precision'], iter_val)\n","        writer.add_scalar('Recall', eval_met['recall'], iter_val)\n","        writer.add_scalar('F1', eval_met['f1'], iter_val)\n","         \n","        \n","        if batch_no % checkpoint_ref == 0:\n","            torch.save(model.state_dict(), f\"./checkpoints/coattention.pth\")\n","            \n","        batch_no += 1\n","        print(f\"Batch -> {batch_no} done -> Accu: {eval_met['accuracy']:.2f} -> Prec: {eval_met['precision']:.2f}\\r\", end=\"\")\n","        avg_accuracy += eval_met['accuracy']\n","        \n","\n","    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Avg. Epoch Acc: {avg_accuracy / len(train_dataloader)}')\n","# writer.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# model.transformer_encoder_text_1.state_dict()['layers.0.self_attn.in_proj_weight']"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5024895,"sourceId":8436330,"sourceType":"datasetVersion"},{"sourceId":178189965,"sourceType":"kernelVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
