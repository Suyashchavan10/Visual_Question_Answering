{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Training Baseline"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T02:14:57.327721Z","iopub.status.busy":"2024-05-17T02:14:57.327397Z","iopub.status.idle":"2024-05-17T02:15:14.403494Z","shell.execute_reply":"2024-05-17T02:15:14.402486Z","shell.execute_reply.started":"2024-05-17T02:14:57.327695Z"},"trusted":true},"outputs":[],"source":["from transformers import BertModel, BertTokenizer, ViTImageProcessor, ViTModel\n","import torch\n","from torchinfo import summary\n","from torch import nn\n","from torch.nn import Transformer, TransformerDecoder, TransformerDecoderLayer, TransformerEncoder, TransformerEncoderLayer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T02:15:20.329181Z","iopub.status.busy":"2024-05-17T02:15:20.328453Z","iopub.status.idle":"2024-05-17T02:15:20.333440Z","shell.execute_reply":"2024-05-17T02:15:20.332402Z","shell.execute_reply.started":"2024-05-17T02:15:20.329153Z"},"trusted":true},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# device = torch.device('cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T02:15:25.482026Z","iopub.status.busy":"2024-05-17T02:15:25.481659Z","iopub.status.idle":"2024-05-17T02:15:25.489602Z","shell.execute_reply":"2024-05-17T02:15:25.488533Z","shell.execute_reply.started":"2024-05-17T02:15:25.481998Z"},"trusted":true},"outputs":[],"source":["device"]},{"cell_type":"markdown","metadata":{},"source":["## Initialize necessary modules"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T02:15:32.140681Z","iopub.status.busy":"2024-05-17T02:15:32.140093Z","iopub.status.idle":"2024-05-17T02:15:32.157244Z","shell.execute_reply":"2024-05-17T02:15:32.156116Z","shell.execute_reply.started":"2024-05-17T02:15:32.140643Z"},"trusted":true},"outputs":[],"source":["class TextTokenizer(torch.nn.Module):\n","    def __init__(\n","        self,\n","        text_tokenizer=BertTokenizer,\n","        max_length=25  # Add a max_length parameter\n","    ):\n","        super().__init__()\n","        self.text_tokenizer = text_tokenizer.from_pretrained('bert-base-uncased')\n","        self.max_length = max_length  # Store the max_length\n","\n","    def forward(self, input_question, padding='max_length', truncation=True):\n","        tokens = self.text_tokenizer(input_question, return_tensors='pt', \n","                                     padding=padding, truncation=truncation, \n","                                     max_length=self.max_length).to(device)  # Use max_length\n","\n","        return tokens\n","\n","class ImageProcessor(torch.nn.Module):\n","    def __init__(\n","        self,\n","        image_model_processor=ViTImageProcessor\n","    ):\n","\n","        super().__init__()\n","        self.image_model_processor = image_model_processor.from_pretrained('google/vit-base-patch16-224-in21k')\n","\n","    def forward(self, image):\n","        image = self.image_model_processor(image, return_tensors='pt').to(device)\n","\n","        return image\n","\n","class TextEmbedding(torch.nn.Module):\n","    def __init__(\n","        self,\n","        text_model=BertModel,\n","    ):\n","        super().__init__()\n","        self.text_model = text_model.from_pretrained('bert-base-uncased').to(device)\n","\n","\n","    def forward(self, tokens):\n","        text_output = self.text_model(input_ids=tokens.input_ids, attention_mask=tokens.attention_mask)\n","        text_output = text_output.last_hidden_state     # CLS token from the last layer\n","\n","        return text_output\n","\n","\n","class ImageEmbedding(torch.nn.Module):\n","    def __init__(\n","            self, \n","            image_model=ViTModel\n","        ):\n","        \n","        super().__init__()\n","        self.image_model = image_model.from_pretrained('google/vit-base-patch16-224-in21k').to(device)\n","\n","\n","    def forward(self, image):\n","        image_output = self.image_model(pixel_values=image.pixel_values).last_hidden_state\n","\n","        return image_output"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T02:15:35.074200Z","iopub.status.busy":"2024-05-17T02:15:35.073458Z","iopub.status.idle":"2024-05-17T02:15:35.083206Z","shell.execute_reply":"2024-05-17T02:15:35.082311Z","shell.execute_reply.started":"2024-05-17T02:15:35.074169Z"},"trusted":true},"outputs":[],"source":["class VQAModel(nn.Module):\n","    def __init__(\n","        self,\n","        dim_model = 768,      # image and text embeddings concatenated\n","        nhead = 12,                    # No. of Attention heads\n","        num_layers = 1,               # No. of encoder layers\n","        num_classes = 8000\n","    ):\n","        super().__init__()\n","        self.text_embedder = TextEmbedding()\n","        self.image_embedder = ImageEmbedding()\n","\n","        encoder_layers = TransformerEncoderLayer(d_model=dim_model, nhead=nhead)\n","        self.transformerEncoder = TransformerEncoder(encoder_layer=encoder_layers, num_layers=num_layers).to(device)\n","\n","        self.classifier = nn.Linear(dim_model, num_classes).to(device)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","\n","    def forward(self, questions, images):\n","        question_embedding = self.text_embedder(questions)\n","        image_embedding = self.image_embedder(images)\n","\n","        embeddings = torch.cat((question_embedding, image_embedding), dim=1)\n","        embeddings = embeddings.permute(1, 0, 2)  # (seq, batch, feature)\n","        output = self.transformerEncoder(embeddings)\n","\n","        cls_output = output[0, :, :]\n","\n","        logits = self.classifier(cls_output)\n","        output = self.softmax(logits)\n","\n","        return output"]},{"cell_type":"markdown","metadata":{},"source":["## Load Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T02:54:29.262300Z","iopub.status.busy":"2024-05-17T02:54:29.261941Z","iopub.status.idle":"2024-05-17T02:58:53.254248Z","shell.execute_reply":"2024-05-17T02:58:53.253450Z","shell.execute_reply.started":"2024-05-17T02:54:29.262275Z"},"trusted":true},"outputs":[],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"HuggingFaceM4/VQAv2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T02:58:53.255944Z","iopub.status.busy":"2024-05-17T02:58:53.255662Z","iopub.status.idle":"2024-05-17T02:58:53.260256Z","shell.execute_reply":"2024-05-17T02:58:53.259333Z","shell.execute_reply.started":"2024-05-17T02:58:53.255919Z"},"trusted":true},"outputs":[],"source":["train_dataset = dataset['train']\n","test_dataset = dataset['test']\n","val_dataset = dataset['validation']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T02:58:53.261701Z","iopub.status.busy":"2024-05-17T02:58:53.261404Z","iopub.status.idle":"2024-05-17T02:58:53.722327Z","shell.execute_reply":"2024-05-17T02:58:53.721545Z","shell.execute_reply.started":"2024-05-17T02:58:53.261678Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","train_df = pd.read_csv('/kaggle/input/vqdata/vqa_train_dataset.csv')\n","val_df = pd.read_csv('/kaggle/input/vqdata/vqa_val_dataset.csv')\n","\n","train_df = train_df[~train_df['answers'].isna()]\n","val_df = val_df[~val_df['answers'].isna()]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T02:58:53.725250Z","iopub.status.busy":"2024-05-17T02:58:53.724887Z","iopub.status.idle":"2024-05-17T02:58:53.751349Z","shell.execute_reply":"2024-05-17T02:58:53.750443Z","shell.execute_reply.started":"2024-05-17T02:58:53.725213Z"},"trusted":true},"outputs":[],"source":["train_df[train_df['answers'].isna()]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T02:58:53.752820Z","iopub.status.busy":"2024-05-17T02:58:53.752511Z","iopub.status.idle":"2024-05-17T02:58:57.514050Z","shell.execute_reply":"2024-05-17T02:58:57.513256Z","shell.execute_reply.started":"2024-05-17T02:58:53.752797Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor\n","\n","class VQADataset(Dataset):\n","    def __init__(self, dataframe, image_dataset):\n","        self.dataframe = dataframe\n","        self.image_dataset = image_dataset\n","        self.text_tokenizer = TextTokenizer()\n","        self.image_processor = ImageProcessor()\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        row = self.dataframe.iloc[idx]\n","        \n","        ind = int(row['index'])\n","        image = self.image_dataset[ind]['image']\n","        question = row['question']\n","        answer = row['answers']\n","        \n","        # sanity check        \n","        assert self.image_dataset[ind]['question'] == question, \"Mismatching training and Image data\"\n","\n","        \n","        image = image.convert('RGB')\n","        \n","#         Preprocessing done in ImageEmbedder\n","#         preprocess = Compose([\n","#             Resize((224, 224)),\n","#             CenterCrop(224),\n","#             ToTensor(),\n","#         ])\n","#         image = preprocess(image)\n","\n","        # Tokenize question\n","        tokens = self.text_tokenizer(question, padding='max_length', truncation=True)\n","        tokens.input_ids = tokens.input_ids.squeeze()\n","        tokens.attention_mask = tokens.attention_mask.squeeze()\n","        image = self.image_processor(image)\n","        return {\n","            'image': image,\n","            'questions': tokens,\n","            'answer': answer\n","        }\n","\n","batch_size = 64\n","    \n","train_data = VQADataset(train_df, train_dataset)\n","val_data = VQADataset(val_df, train_dataset)\n","\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T02:58:57.515417Z","iopub.status.busy":"2024-05-17T02:58:57.515136Z","iopub.status.idle":"2024-05-17T02:58:57.532515Z","shell.execute_reply":"2024-05-17T02:58:57.531738Z","shell.execute_reply.started":"2024-05-17T02:58:57.515382Z"},"trusted":true},"outputs":[],"source":["import pickle\n","\n","with open('/kaggle/input/vqdata/answers_dictionaries.pkl', 'rb') as f:\n","    data = pickle.load(f)\n","    id_to_answer = data['id_to_answer']\n","    answer_to_id = data['answer_to_id']\n","\n","# print(\"Dictionaries have been loaded from answers_dictionaries.pkl\")\n","# print(\"ID to Answer Dictionary:\", id_to_answer)\n","# print(\"Answer dede please: \",Â answer_to_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T02:58:57.533695Z","iopub.status.busy":"2024-05-17T02:58:57.533455Z","iopub.status.idle":"2024-05-17T02:59:17.727349Z","shell.execute_reply":"2024-05-17T02:59:17.726361Z","shell.execute_reply.started":"2024-05-17T02:58:57.533674Z"},"trusted":true},"outputs":[],"source":["assert len(answer_to_id) == len(id_to_answer)\n","model = VQAModel(num_classes=len(answer_to_id))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T02:59:17.728870Z","iopub.status.busy":"2024-05-17T02:59:17.728577Z","iopub.status.idle":"2024-05-17T02:59:17.751365Z","shell.execute_reply":"2024-05-17T02:59:17.750416Z","shell.execute_reply.started":"2024-05-17T02:59:17.728845Z"},"trusted":true},"outputs":[],"source":["train_df[train_df['answers'].isna()]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T02:59:17.752957Z","iopub.status.busy":"2024-05-17T02:59:17.752615Z","iopub.status.idle":"2024-05-17T02:59:17.791115Z","shell.execute_reply":"2024-05-17T02:59:17.790221Z","shell.execute_reply.started":"2024-05-17T02:59:17.752925Z"},"trusted":true},"outputs":[],"source":["from torch.utils.tensorboard import SummaryWriter\n","\n","# Create a SummaryWriter object\n","writer = SummaryWriter('runs/experiment_2_final')\n","num_epochs = 3\n","model.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T02:59:17.793896Z","iopub.status.busy":"2024-05-17T02:59:17.793618Z","iopub.status.idle":"2024-05-17T02:59:17.800948Z","shell.execute_reply":"2024-05-17T02:59:17.800068Z","shell.execute_reply.started":"2024-05-17T02:59:17.793873Z"},"trusted":true},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.003)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T02:59:17.802338Z","iopub.status.busy":"2024-05-17T02:59:17.802016Z","iopub.status.idle":"2024-05-17T02:59:17.810174Z","shell.execute_reply":"2024-05-17T02:59:17.809335Z","shell.execute_reply.started":"2024-05-17T02:59:17.802313Z"},"trusted":true},"outputs":[],"source":["checkpoint_ref = 100"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T02:59:17.811881Z","iopub.status.busy":"2024-05-17T02:59:17.811307Z","iopub.status.idle":"2024-05-17T02:59:19.918608Z","shell.execute_reply":"2024-05-17T02:59:19.917538Z","shell.execute_reply.started":"2024-05-17T02:59:17.811850Z"},"trusted":true},"outputs":[],"source":["from torchmetrics.classification import Precision, Recall, Accuracy, F1Score, AUROC"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T02:59:19.920134Z","iopub.status.busy":"2024-05-17T02:59:19.919839Z","iopub.status.idle":"2024-05-17T02:59:19.933476Z","shell.execute_reply":"2024-05-17T02:59:19.932113Z","shell.execute_reply.started":"2024-05-17T02:59:19.920109Z"},"trusted":true},"outputs":[],"source":["precision_metric = Precision(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n","recall_metric = Recall(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n","accuracy_metric = Accuracy(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n","f1_metric = F1Score(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n","# auroc_metric = AUROC(task=\"multiclass\", num_classes=len(answer_to_id)).to(device)\n","\n","def evaluate(preds, true):\n","    p = precision_metric(preds, true)\n","    r = recall_metric(preds, true)\n","    a = accuracy_metric(preds, true)\n","    f = f1_metric(preds, true)\n","#     am = auroc_metric(preds, true)\n","    \n","    return {\n","        \"precision\": p,\n","        \"recall\": r,\n","        \"accuracy\": a,\n","        \"f1\": f,\n","#         \"auroc\": auroc_metric\n","    }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T02:59:19.935701Z","iopub.status.busy":"2024-05-17T02:59:19.935161Z","iopub.status.idle":"2024-05-17T02:59:21.036034Z","shell.execute_reply":"2024-05-17T02:59:21.034639Z","shell.execute_reply.started":"2024-05-17T02:59:19.935667Z"},"trusted":true},"outputs":[],"source":["!mkdir ./checkpoints"]},{"cell_type":"markdown","metadata":{},"source":["## Training Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T06:37:42.097084Z","iopub.status.busy":"2024-05-17T06:37:42.096095Z","iopub.status.idle":"2024-05-17T06:37:43.883401Z","shell.execute_reply":"2024-05-17T06:37:43.881679Z","shell.execute_reply.started":"2024-05-17T06:37:42.097037Z"},"trusted":true},"outputs":[],"source":["import torch.nn.functional as F\n","\n","batch_no = 0\n","avg_accuracy = 0\n","\n","for epoch in range(num_epochs):\n","    batch_no = 0\n","    avg_accuracy = 0\n","    for batch in train_dataloader:\n","        # Get the inputs and targets from the batch\n","        images = batch['image']\n","        questions = batch['questions']\n","        answers = batch['answer']\n","\n","        questions.input_ids = questions.input_ids.squeeze()\n","        questions.attention_mask = questions.attention_mask.squeeze()\n","        images.pixel_values = images.pixel_values.squeeze()\n","        \n","        # Forward pass\n","        outputs = model(questions, images)\n","\n","        \n","        answers_ids = [answer_to_id[key] for key in answers]\n","        answers_ids_tensor = torch.tensor(answers_ids)\n","        answers = F.one_hot(answers_ids_tensor, num_classes=len(answer_to_id)).to(outputs.dtype)\n","        answers = answers.to(device)\n","        \n","#         answers_ids = [answer_to_id[key] for key in answers]\n","#         answers = torch.zeros(batch_size, len(answer_to_id))\n","#         for i in range(batch_size):\n","#             answers[i, answers_ids[i]] = 1\n","        \n","        # Sanity check\n","        assert answers.shape == outputs.shape, \"Target and Predicted shapes don't match\"\n","    \n","        # Compute the loss\n","        loss = criterion(outputs, answers)\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        \n","        _, preds = torch.max(outputs, 1)\n","        _, answer_inds = torch.max(answers, 1)\n","        \n","        assert preds.shape == answer_inds.shape, f\"Preds_shape: {preds.shape}, Answers_shape: {answer_inds.shape}\"\n","        eval_met = evaluate(preds, answer_inds)\n","        \n","        iter_val = epoch * len(train_dataloader) + batch_no\n","        \n","        \n","        \n","        writer.add_scalar('Training Loss', loss.item(), iter_val)\n","        writer.add_pr_curve('PR Curve', answers, outputs, iter_val)\n","        writer.add_scalar('Accuracy', eval_met['accuracy'], iter_val)\n","        writer.add_scalar('Precision', eval_met['precision'], iter_val)\n","        writer.add_scalar('Recall', eval_met['recall'], iter_val)\n","        writer.add_scalar('F1', eval_met['f1'], iter_val)\n","        \n","        \n","        \n","        if batch_no % checkpoint_ref == 0:\n","            torch.save(model.state_dict(), f\"./checkpoints/latest.pth\")\n","            \n","        batch_no += 1\n","        print(f\"Batch -> {batch_no} done -> Accu: {eval_met['accuracy']:.2f} -> Prec: {eval_met['precision']:.2f}\\r\", end=\"\")\n","        avg_accuracy += eval_met['accuracy']\n","        \n","\n","    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Avg. Epoch Acc: {avg_accuracy / len(train_dataloader)}')\n","# writer.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T06:37:52.778031Z","iopub.status.busy":"2024-05-17T06:37:52.776919Z","iopub.status.idle":"2024-05-17T06:37:52.782771Z","shell.execute_reply":"2024-05-17T06:37:52.781859Z","shell.execute_reply.started":"2024-05-17T06:37:52.777996Z"},"trusted":true},"outputs":[],"source":["writer.close()"]},{"cell_type":"markdown","metadata":{},"source":["### Model Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["LATEST_CHECKPOINT = './checkpoints/.pth'\n","\n","val_model = VQAModel(num_classes=len(answer_to_id))\n","state_dict = torch.load(LATEST_CHECKPOINT)\n","\n","val_model.load_state_dict(state_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["val_writer = SummaryWriter('/runs/experiment_1_val')\n","iter_val = 0\n","total_loss = 0\n","val_model.eval()\n","\n","with torch.no_grad():\n","    for batch in val_dataloader:\n","        images = batch['image']\n","        questions = batch['questions']\n","        answers = batch['answer']\n","\n","        questions.input_ids = questions.input_ids.squeeze()\n","        questions.attention_mask = questions.attention_mask.squeeze()\n","        images.pixel_values = images.pixel_values.squeeze()\n","        \n","        # Forward pass\n","        outputs = val_model(questions, images)\n","\n","        \n","        answers_ids = [answer_to_id[key] for key in answers]\n","        answers_ids_tensor = torch.tensor(answers_ids)\n","        answers = F.one_hot(answers_ids_tensor, num_classes=len(answer_to_id)).to(outputs.dtype)\n","        answers = answers.to(device)\n","        \n","#         answers_ids = [answer_to_id[key] for key in answers]\n","#         answers = torch.zeros(batch_size, len(answer_to_id))\n","#         for i in range(batch_size):\n","#             answers[i, answers_ids[i]] = 1\n","        \n","        # Sanity check\n","        assert answers.shape == outputs.shape, \"Target and Predicted shapes don't match\"\n","    \n","        # Compute the loss\n","        loss = criterion(outputs, answers)\n","\n","        \n","        _, preds = torch.max(outputs, 1)\n","        _, answer_inds = torch.max(answers, 1)\n","        \n","        assert preds.shape == answer_inds.shape, f\"Preds_shape: {preds.shape}, Answers_shape: {answer_inds.shape}\"\n","        eval_met = evaluate(preds, answer_inds)\n","        \n","        iter_val = batch_no\n","        \n","        \n","        \n","        val_writer.add_scalar('Validation Loss', loss.item(), iter_val)\n","        val_writer.add_pr_curve('PR Curve', answers, outputs, iter_val)\n","        val_writer.add_scalar('Accuracy', eval_met['accuracy'], iter_val)\n","        val_writer.add_scalar('Precision', eval_met['precision'], iter_val)\n","        val_writer.add_scalar('Recall', eval_met['recall'], iter_val)\n","        val_writer.add_scalar('F1', eval_met['f1'], iter_val)\n","        \n","            \n","        batch_no += 1\n","        print(f\"Batch -> {batch_no} done -> Accu: {eval_met['accuracy']:.2f} -> Prec: {eval_met['precision']:.2f}\\r\", end=\"\")\n","        avg_accuracy += eval_met['accuracy']\n","        total_loss += loss.item()\n","\n","# Compute the average loss and the accuracy\n","avg_loss = total_loss / len(val_dataloader)\n","accuracy = avg_accuracy / len(val_dataloader)\n","val_writer.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T06:47:00.172484Z","iopub.status.busy":"2024-05-17T06:47:00.171669Z","iopub.status.idle":"2024-05-17T06:47:01.259606Z","shell.execute_reply":"2024-05-17T06:47:01.258516Z","shell.execute_reply.started":"2024-05-17T06:47:00.172454Z"},"trusted":true},"outputs":[],"source":["# !pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(f\"Average validation Loss: {avg_loss}\")\n","print(f\"Average accuracy: {accuracy}\")"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5020279,"sourceId":8430164,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
